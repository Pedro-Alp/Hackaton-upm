{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13601163,"sourceType":"datasetVersion","datasetId":8642787}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üè• BioClinicalBERT + Machine Learning para Predicci√≥n de Diabetes\n## Hackathon HackUPM 2025 - Notebook Completo y Funcional\n\n**Pipeline completo:**\n1. Cargar train.json + test.json\n2. Extraer edad, g√©nero, features cl√≠nicos (con regex robustas y negaciones)\n3. Generar embeddings con BioClinicalBERT (768-dim)\n4. Agrupar por paciente (media de features + embeddings)\n5. EDA completo sin errores\n6. Modelado con RandomForest\n7. Predicciones finales en test\n8. Guardado en m√∫ltiples formatos\n\n**Autor:** Pipeline integrado de IA  \n**Fecha:** 03-11-2025\n","metadata":{}},{"cell_type":"code","source":"# !pip install word2number\n# !pip install --upgrade git+https://github.com/huggingface/transformers.git ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:18:07.176803Z","iopub.execute_input":"2025-11-04T16:18:07.177530Z","iopub.status.idle":"2025-11-04T16:18:07.180566Z","shell.execute_reply.started":"2025-11-04T16:18:07.177504Z","shell.execute_reply":"2025-11-04T16:18:07.179941Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# INSTALACIONES (ejecutar si es primera vez, descomentar)\n# !pip install -q transformers torch pandas numpy tqdm scikit-learn\n\nimport json\nimport pandas as pd\nimport numpy as np\nimport torch\nimport re\nfrom word2number import w2n\nimport warnings\nfrom transformers import AutoTokenizer, AutoModel\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n\nwarnings.filterwarnings('ignore')\n\nprint(\"‚úÖ Librer√≠as importadas\")\nprint(f\"‚úÖ CUDA disponible: {torch.cuda.is_available()}\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"‚úÖ Dispositivo: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:18:07.196017Z","iopub.execute_input":"2025-11-04T16:18:07.196210Z","iopub.status.idle":"2025-11-04T16:18:07.202282Z","shell.execute_reply.started":"2025-11-04T16:18:07.196195Z","shell.execute_reply":"2025-11-04T16:18:07.201570Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Librer√≠as importadas\n‚úÖ CUDA disponible: True\n‚úÖ Dispositivo: cuda\n","output_type":"stream"}],"execution_count":57},{"cell_type":"markdown","source":"## 1Ô∏è‚É£ Cargar Datos (train.json + test.json)","metadata":{}},{"cell_type":"code","source":"print(\"=\" * 80)\nprint(\"CARGANDO DATOS\")\nprint(\"=\" * 80)\n\nprint(\"\\nüì• Leyendo train.json...\")\nwith open(\"/kaggle/input/hackathon-dataset/train.json\", \"r\") as f:\n    train_data = json.load(f)\n\nprint(\"üì• Leyendo test.json...\")\nwith open(\"/kaggle/input/hackathon-dataset/test.json\", \"r\") as f:\n    test_data = json.load(f)\n\n# Crear DataFrames iniciales\ndf_train_raw = pd.DataFrame(train_data)\ndf_test_raw = pd.DataFrame(test_data)\n\nprint(f\"\\n‚úÖ Train: {len(df_train_raw)} registros, {df_train_raw.shape[1]} columnas\")\nprint(f\"‚úÖ Test: {len(df_test_raw)} registros, {df_test_raw.shape[1]} columnas\")\nprint(f\"\\nüìä Distribuci√≥n diabetes en TRAIN:\")\nprint(df_train_raw[\"has_diabetes\"].value_counts())\nprint(f\"Proporci√≥n positivos: {df_train_raw['has_diabetes'].mean():.2%}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:18:07.203427Z","iopub.execute_input":"2025-11-04T16:18:07.203675Z","iopub.status.idle":"2025-11-04T16:18:07.250942Z","shell.execute_reply.started":"2025-11-04T16:18:07.203659Z","shell.execute_reply":"2025-11-04T16:18:07.250361Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nCARGANDO DATOS\n================================================================================\n\nüì• Leyendo train.json...\nüì• Leyendo test.json...\n\n‚úÖ Train: 3000 registros, 3 columnas\n‚úÖ Test: 300 registros, 2 columnas\n\nüìä Distribuci√≥n diabetes en TRAIN:\nhas_diabetes\n0    2100\n1     900\nName: count, dtype: int64\nProporci√≥n positivos: 30.00%\n","output_type":"stream"}],"execution_count":58},{"cell_type":"markdown","source":"## 2Ô∏è‚É£ Funciones de Extracci√≥n (Robustas con Negaciones)","metadata":{}},{"cell_type":"code","source":"outliers = {\n    \"age\": [],\n    \"glucose\": [],\n    \"hba1c\": [],\n    \"bmi\": []\n}\n\ndef safe_float(x):\n    \"\"\"Convierte a float de forma segura.\"\"\"\n    try:\n        return float(x)\n    except:\n        return np.nan\n\ndef extract_age(text):\n    \"\"\"Extrae edad con rango v√°lido 0-120.\"\"\"\n    if not isinstance(text, str):\n        return np.nan\n    t = text.lower()\n    # Patr√≥n 1: \"X-year-old\" o \"age X\"\n    m = re.search(r'(\\d{1,3})\\s*(?:year)?-?\\s*(?:year-old|yr|years?\\s*old)', t)\n    age = None\n    if not m:\n        m = re.search(r'(?:age|aged)\\s*(?:is)?\\s*(\\d{1,3})\\b', t)\n        if not m:\n            m = re.search(r\"\\b([a-zA-Z]+(?:[-\\s][a-zA-Z]+)*)-year-old\\b\", t)\n            if m:\n                age = w2n.word_to_num(m.group(1))\n    if not age:\n        age = int(m.group(1)) if m else np.nan\n    if not m:\n        outliers[\"age\"].append(t)\n    return age if (not np.isnan(age) and 0 <= age <= 120) else np.nan\n\ndef extract_gender(text):\n    \"\"\"Extrae g√©nero (male/female/unknown).\"\"\"\n    if not isinstance(text, str):\n        return \"unknown\"\n    t = text.lower()\n    male_count = len(re.findall(r'\\b(?:male|man|he|his|him|boy)\\b', t))\n    female_count = len(re.findall(r'\\b(?:female|woman|she|her|girl)\\b', t))\n\n    if male_count > 0 and female_count == 0:\n        return \"male\"\n    elif female_count > 0 and male_count == 0:\n        return \"female\"\n    elif male_count == female_count == 0:\n        return \"unknown\"\n    else:\n        return \"male\" if male_count >= female_count else \"female\"\n\ndef extract_bmi(text):\n    \"\"\"Extrae BMI con rango v√°lido 8-80.\"\"\"\n    if not isinstance(text, str):\n        return np.nan\n    t = text.lower()\n    m = re.search(r'\\b(?:bmi|imc)\\b[^0-9]{0,30}(\\d{1,3}(?:\\.\\d+)?)', t)\n    if not m:\n        m = re.search(r'\\b(?:bmi|imc)\\b.{0,26}range.{0,10}(\\d{1,3}(?:[.,]\\d+)?)', t)\n    v = safe_float(m.group(1)) if m else np.nan\n    if not m:\n        outliers[\"bmi\"].append(t)\n    return v if (not np.isnan(v) and 0 <= v <= 80) else np.nan\n\ndef extract_hba1c(text):\n    \"\"\"Extrae HbA1c con rango v√°lido 3-20.\"\"\"\n    if not isinstance(text, str):\n        return np.nan\n    t = text.lower()\n    # Ventana local: hasta 20 chars despu√©s de \"hba1c\"\n    m = re.search(r'(?:hba1c|a1c)[^0-9]{0,20}(\\d{1,2}(?:\\.\\d+)?)\\s*%?', t)\n    if not m:\n        pattern = re.compile(\n            r'\\b(?:hba1c(?: level)?s?\\b.{0,10}(?:is|are|was|were|within|of)?\\s*(very\\s+high|high|elevated|normal|within normal limits|low)|(very\\s+high|high|elevated|normal|within normal limits|low)\\s+(?:levels\\s+of\\s+)?hba1c)\\b',\n            re.IGNORECASE\n        )\n        m = pattern.search(t)\n        if m:\n            mapping = {\n                \"normal\": 5.5,\n                \"elevated\": 6.3,\n                \"high\": 6.5,\n                \"very high\": 8\n                }\n            v = mapping[m.group(1) or m.group(2)] if m else np.nan\n            return v if (not np.isnan(v) and 0 <= v <= 20) else np.nan\n    if not m:\n        outliers[\"hba1c\"].append(t)\n    \n    v = safe_float(m.group(1)) if m else np.nan\n    return v if (not np.isnan(v) and 0 <= v <= 20) else np.nan\n\ndef extract_glucose(text):\n    \"\"\"Extrae glucosa (aleatoria o postprandial) con rango v√°lido 40-600.\"\"\"\n    if not isinstance(text, str):\n        return np.nan\n    t = text.lower()\n    # Preferir \"glucose\" + 2-3 d√≠gitos\n    m = re.search(r'\\bglucose\\b[^0-9]{0,20}(\\d{2,3})(?:\\s*mg/dl)?', t)\n    v = safe_float(m.group(1)) if m else np.nan\n    if not m:\n        pattern = re.compile(\n            r'\\b(?:'\n            r'(?:\\w+\\s+)?glucose(?: (?:level|levels|measurement|measurements|reading|readings))?\\b.{0,10}(?:is|are|was|were|within|of)?\\s*(?P<val>very\\s+high|high|elevated|normal|within normal limits|low|decreased|reduced|abnormal)'\n            r'|'\n            r'(?P<val2>very\\s+high|high|elevated|normal|within normal limits|low|decreased|reduced|abnormal)\\s+(?:\\w+\\s+)?glucose(?: (?:level|levels|measurement|measurements|reading|readings))?'\n            r')\\b',\n            re.IGNORECASE\n        )\n        m = pattern.search(t)\n        if m:\n            mapping = {\n                \"low\": 70,\n                \"normal\": 140,\n                \"within normal limits\": 140,\n                \"elevated\": 165,\n                \"high\": 200,\n                \"abnormal\": 200,\n                \"very high\": 250,\n                }\n            try:\n                v = mapping[m.group(1) or m.group(2)] if m else np.nan\n            except:\n                print(t)\n                exit(0)\n            return v if (not np.isnan(v)) else np.nan\n    if not m:\n        outliers[\"glucose\"].append(t)\n    return v if (not np.isnan(v)) else np.nan\n\ndef extract_flags(text):\n    \"\"\"Extrae hipertensi√≥n, cardiopat√≠a, fumaci√≥n (respeta negaciones).\"\"\"\n    if not isinstance(text, str):\n        return 0, 0, \"unknown\"\n\n    t = text.lower()\n    NEG_PAT = r'(?:no\\s+|without\\s+|denies\\s+|negative\\s+for\\s+|no\\s+history\\s+of\\s+)'\n\n    # Hipertensi√≥n: negaci√≥n > pos\n    hyp_neg = bool(re.search(NEG_PAT + r'(?:hypertension|high\\s+blood\\s+pressure)', t))\n    hyp_pos = bool(re.search(r'\\bhypertension\\b|\\bhigh\\s+blood\\s+pressure\\b', t)) and not hyp_neg\n    has_hypertension = 1 if hyp_pos else 0\n\n    # Cardiopat√≠a: negaci√≥n > pos\n    hd_neg = bool(re.search(NEG_PAT + r'(?:heart\\s+disease|cardiovascular)', t))\n    hd_pos = bool(re.search(r'\\bheart\\s+disease\\b|\\bcardiovascular', t)) and not hd_neg\n    has_heart_disease = 1 if hd_pos else 0\n\n    # Fumaci√≥n\n    if re.search(r'\\bnon-smoker\\b|\\bnever\\s+smoked\\b', t):\n        smoking = \"never\"\n    elif re.search(r'\\b(?:past|former)\\s+(?:smoker|smoking)\\b', t):\n        smoking = \"past\"\n    elif re.search(r'\\bcurrent\\s+smoker\\b|\\bis\\s+a\\s+smoker\\b|\\bsmoker\\b', t):\n        smoking = \"current\"\n    else:\n        smoking = \"unknown\"\n\n    return has_hypertension, has_heart_disease, smoking\n\n# TEST: verificar extracci√≥n en muestra\nimport numpy as np\n\nfor i in range(min(3, len(df_train_raw))):\n    note = df_train_raw[\"medical_note\"].iloc[i]\n    age = extract_age(note)\n    gender = extract_gender(note)\n    bmi = extract_bmi(note)\n    hba1c = extract_hba1c(note)\n    glucose = extract_glucose(note)\n    hyp, hd, smoking = extract_flags(note)\n\n    age_s    = \"NaN\" if (age is None or (isinstance(age, float) and np.isnan(age))) else f\"{int(age)}\"\n    bmi_s    = \"NaN\" if (bmi is None or np.isnan(bmi)) else f\"{bmi:.1f}\"\n    hba1c_s  = \"NaN\" if (hba1c is None or np.isnan(hba1c)) else f\"{hba1c:.1f}\"\n    glucose_s= \"NaN\" if (glucose is None or np.isnan(glucose)) else f\"{glucose:.0f}\"\n\n    print(\n        f\"\\n  [{i}] age={age_s}, gender={gender}, bmi={bmi_s}, \"\n        f\"hba1c={hba1c_s}, glucose={glucose_s}, hyp={hyp}, hd={hd}, smoking={smoking}\"\n    )\n\n\nprint(\"\\n‚úÖ Extracci√≥n verificada\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:18:07.311527Z","iopub.execute_input":"2025-11-04T16:18:07.311938Z","iopub.status.idle":"2025-11-04T16:18:07.331485Z","shell.execute_reply.started":"2025-11-04T16:18:07.311919Z","shell.execute_reply":"2025-11-04T16:18:07.330650Z"}},"outputs":[{"name":"stdout","text":"\n  [0] age=16, gender=female, bmi=21.5, hba1c=6.2, glucose=140, hyp=0, hd=1, smoking=never\n\n  [1] age=15, gender=female, bmi=33.6, hba1c=5.5, glucose=158, hyp=1, hd=1, smoking=unknown\n\n  [2] age=54, gender=male, bmi=21.5, hba1c=5.5, glucose=145, hyp=0, hd=1, smoking=current\n\n‚úÖ Extracci√≥n verificada\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"# Aplicar a TRAIN\nprint(\"\\n Train: extrayendo features...\")\ndf_train_raw[\"age\"] = df_train_raw[\"medical_note\"].apply(extract_age)\ndf_train_raw[\"gender\"] = df_train_raw[\"medical_note\"].apply(extract_gender)\ndf_train_raw[\"bmi\"] = df_train_raw[\"medical_note\"].apply(extract_bmi)\ndf_train_raw[\"hba1c\"] = df_train_raw[\"medical_note\"].apply(extract_hba1c)\ndf_train_raw[\"glucose\"] = df_train_raw[\"medical_note\"].apply(extract_glucose)\n\ntmp_train = df_train_raw[\"medical_note\"].apply(extract_flags)\ndf_train_raw[\"has_hypertension\"] = [t[0] for t in tmp_train]\ndf_train_raw[\"has_heart_disease\"] = [t[1] for t in tmp_train]\ndf_train_raw[\"smoking_status\"] = [t[2] for t in tmp_train]\n\n# Aplicar a TEST\nprint(\" Test: extrayendo features...\")\ndf_test_raw[\"age\"] = df_test_raw[\"medical_note\"].apply(extract_age)\ndf_test_raw[\"gender\"] = df_test_raw[\"medical_note\"].apply(extract_gender)\ndf_test_raw[\"bmi\"] = df_test_raw[\"medical_note\"].apply(extract_bmi)\ndf_test_raw[\"hba1c\"] = df_test_raw[\"medical_note\"].apply(extract_hba1c)\ndf_test_raw[\"glucose\"] = df_test_raw[\"medical_note\"].apply(extract_glucose)\n\ntmp_test = df_test_raw[\"medical_note\"].apply(extract_flags)\ndf_test_raw[\"has_hypertension\"] = [t[0] for t in tmp_test]\ndf_test_raw[\"has_heart_disease\"] = [t[1] for t in tmp_test]\ndf_test_raw[\"smoking_status\"] = [t[2] for t in tmp_test]\n\nprint(\"\\n Features extra√≠dos correctamente\")\nprint(f\"\\n MUESTRA TRAIN (primeras 3 filas):\")\ncols = ['patient_id', 'age', 'gender', 'bmi', 'hba1c', 'glucose', 'smoking_status', 'has_diabetes']\nprint(df_train_raw[cols].head(3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:18:07.332695Z","iopub.execute_input":"2025-11-04T16:18:07.332973Z","iopub.status.idle":"2025-11-04T16:18:07.756976Z","shell.execute_reply.started":"2025-11-04T16:18:07.332956Z","shell.execute_reply":"2025-11-04T16:18:07.756303Z"}},"outputs":[{"name":"stdout","text":"\n Train: extrayendo features...\n Test: extrayendo features...\n\n Features extra√≠dos correctamente\n\n MUESTRA TRAIN (primeras 3 filas):\n   patient_id   age  gender    bmi  hba1c  glucose smoking_status  \\\n0       82555  16.0  female  21.49    6.2    140.0          never   \n1       92299  15.0  female  33.62    5.5    158.0        unknown   \n2       18725  54.0    male  21.46    5.5    145.0        current   \n\n   has_diabetes  \n0             0  \n1             0  \n2             0  \n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"df_train_raw.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:18:07.757740Z","iopub.execute_input":"2025-11-04T16:18:07.758004Z","iopub.status.idle":"2025-11-04T16:18:07.764999Z","shell.execute_reply.started":"2025-11-04T16:18:07.757985Z","shell.execute_reply":"2025-11-04T16:18:07.764326Z"}},"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"patient_id             0\nhas_diabetes           0\nmedical_note           0\nage                    9\ngender                 0\nbmi                    5\nhba1c                115\nglucose              211\nhas_hypertension       0\nhas_heart_disease      0\nsmoking_status         0\ndtype: int64"},"metadata":{}}],"execution_count":61},{"cell_type":"markdown","source":"## 3Ô∏è‚É£ BioClinicalBERT: Generar Embeddings (768-dim)","metadata":{}},{"cell_type":"code","source":"def load_bioclinicalbert():\n    \"\"\"Carga Bio_ClinicalBERT desde HuggingFace.\"\"\"\n    model_name = \"emilyalsentzer/Bio_ClinicalBERT\" # coge el modelo de HuggingFace\n    print(f\" Cargando {model_name}...\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name) # convierte el texto en tokens\n    model = AutoModel.from_pretrained(model_name) # Genera los embeddings desde la red a los token de la secuencia\n    model.eval() # evalua que se ha pasado de texto a embeddings\n    model.to(device) # mueve el modelo a GPU (kaggle)\n    return tokenizer, model\n\ndef mean_pool(last_hidden_state, attention_mask):\n    \"\"\"Mean pooling con mask.\"\"\"\n    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float() # mascara que ve cuales partes de los valores del vector influyeno o no \n    sum_hidden = (last_hidden_state * mask).sum(dim=1) # quita los insignificantes y sumas todos los buenos\n    sum_mask = torch.clamp(mask.sum(dim=1), min=1e-9) # clump evita que se divida en la siguiente linea entre 0 \n    return sum_hidden / sum_mask # calcula la media de todos los tokens entre los de la mascara\n\ndef embed_text(text, tokenizer, model, max_length=512):\n    \"\"\"Genera 1 embedding de 768 dims para un texto si esta vacia o no es texto.\"\"\"\n    if not isinstance(text, str) or len(text.strip()) == 0: \n        return np.zeros(768, dtype=np.float32) \n\n    tokens = tokenizer(text,padding=True,truncation=True,max_length=max_length,return_tensors=\"pt\").to(device) # crea los tokens desde la frase\n\n    with torch.no_grad(): # sin gradientes para no entrenar modelo\n        output = model(**tokens) # crea los embeddings\n        pooled = mean_pool(output.last_hidden_state, tokens[\"attention_mask\"]) # llama a la funcion de la mascara para ponderar correctamente\n\n    return pooled.cpu().numpy()[0].astype(np.float32)\n\n# Cargar modelo (una sola vez)\ntokenizer, model = load_bioclinicalbert()\n\n# Generar embeddings TRAIN\nprint(\"\\n Generando embeddings TRAIN...\")\ntrain_embeddings = []\nfor note in tqdm(df_train_raw[\"medical_note\"].tolist(), desc=\"Train embeddings\", total=len(df_train_raw)):\n    emb = embed_text(note, tokenizer, model)\n    train_embeddings.append(emb)\n\ndf_train_raw[\"embedding\"] = train_embeddings\n\n# Generar embeddings TEST\nprint(\"\\n Generando embeddings TEST...\")\ntest_embeddings = []\nfor note in tqdm(df_test_raw[\"medical_note\"].tolist(), desc=\"Test embeddings\", total=len(df_test_raw)):\n    emb = embed_text(note, tokenizer, model)\n    test_embeddings.append(emb)\n\ndf_test_raw[\"embedding\"] = test_embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:18:07.766458Z","iopub.execute_input":"2025-11-04T16:18:07.766678Z","iopub.status.idle":"2025-11-04T16:18:42.498297Z","shell.execute_reply.started":"2025-11-04T16:18:07.766663Z","shell.execute_reply":"2025-11-04T16:18:42.497692Z"}},"outputs":[{"name":"stdout","text":" Cargando emilyalsentzer/Bio_ClinicalBERT...\n\n Generando embeddings TRAIN...\n","output_type":"stream"},{"name":"stderr","text":"Train embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:30<00:00, 99.44it/s] \n","output_type":"stream"},{"name":"stdout","text":"\n Generando embeddings TEST...\n","output_type":"stream"},{"name":"stderr","text":"Test embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:03<00:00, 99.70it/s] \n","output_type":"stream"}],"execution_count":62},{"cell_type":"markdown","source":"## 4Ô∏è‚É£ Agrupar por Paciente + Expandir Embeddings","metadata":{}},{"cell_type":"code","source":"def most_common(series):\n    \"\"\"Retorna valor m√°s frecuente o 'unknown' del embedding.\"\"\"\n    s = series.dropna()\n    return s.mode().iat[0] if not s.mode().empty else \"unknown\"\n\ndef emb_mean(series):\n    \"\"\"Promedia embeddings.\"\"\"\n    stacked = np.vstack(series.values)\n    return stacked.mean(axis=0)\n\n\n# Agregaci√≥n TRAIN\nprint(\"\\n Agrupando TRAIN por patient_id...\")\nagg_dict = {\n    \"medical_note\": \"count\",\n    \"has_diabetes\": \"first\",\n    \"age\": \"mean\",\n    \"gender\": most_common,\n    \"bmi\": \"mean\",\n    \"hba1c\": \"mean\",\n    \"glucose\": \"mean\",\n    \"has_hypertension\": \"max\",\n    \"has_heart_disease\": \"max\",\n    \"smoking_status\": most_common,\n    \"embedding\": emb_mean\n}\n\ndf_train_agg = df_train_raw.groupby(\"patient_id\").agg(agg_dict).reset_index() # por si se repiten pacientes, agruparlos\ndf_train_agg.rename(columns={\"medical_note\": \"note_count\"}, inplace=True)\n\nprint(f\" Train agrupado: {df_train_agg.shape[0]} pacientes √∫nicos\")\n\n# Agregaci√≥n TEST (sin has_diabetes)\nprint(\"\\n Agrupando TEST por patient_id...\")\nagg_dict_test = {\n    \"medical_note\": \"count\",\n    \"age\": \"mean\",\n    \"gender\": most_common,\n    \"bmi\": \"mean\",\n    \"hba1c\": \"mean\",\n    \"glucose\": \"mean\",\n    \"has_hypertension\": \"max\",\n    \"has_heart_disease\": \"max\",\n    \"smoking_status\": most_common,\n    \"embedding\": emb_mean\n}\n\ndf_test_agg = df_test_raw.groupby(\"patient_id\").agg(agg_dict_test).reset_index()\ndf_test_agg.rename(columns={\"medical_note\": \"note_count\"}, inplace=True)\n\nprint(f\" Test agrupado: {df_test_agg.shape[0]} pacientes √∫nicos\")\n\n# Expandir embeddings en columnas\nprint(\"\\n Expandiendo embeddings (768 columnas)...\")\n\nemb_train = np.vstack(df_train_agg[\"embedding\"].values)\nemb_test = np.vstack(df_test_agg[\"embedding\"].values)\n\n# Crea los embeddings (columnas) en el Dataframe\nemb_cols = [f\"emb_{i}\" for i in range(emb_train.shape[1])]\nemb_df_train = pd.DataFrame(emb_train, columns=emb_cols)\nemb_df_test = pd.DataFrame(emb_test, columns=emb_cols)\n\ndf_train_final = pd.concat([df_train_agg.drop(columns=[\"embedding\"]).reset_index(drop=True), emb_df_train], axis=1)\ndf_test_final = pd.concat([df_test_agg.drop(columns=[\"embedding\"]).reset_index(drop=True), emb_df_test], axis=1)\n\nprint(f\"\\n Train final: {df_train_final.shape}\")\nprint(f\" Test final: {df_test_final.shape}\")\n\nprint(f\"\\n PRIMERAS FILAS TRAIN (con age y gender):\")\nprint(df_train_final[['patient_id', 'age', 'gender', 'bmi', 'hba1c', 'glucose', 'smoking_status', 'has_diabetes', 'note_count']].head(5))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:18:42.499152Z","iopub.execute_input":"2025-11-04T16:18:42.499444Z","iopub.status.idle":"2025-11-04T16:18:44.236028Z","shell.execute_reply.started":"2025-11-04T16:18:42.499420Z","shell.execute_reply":"2025-11-04T16:18:44.235181Z"}},"outputs":[{"name":"stdout","text":"\n Agrupando TRAIN por patient_id...\n Train agrupado: 3000 pacientes √∫nicos\n\n Agrupando TEST por patient_id...\n Test agrupado: 300 pacientes √∫nicos\n\n Expandiendo embeddings (768 columnas)...\n\n Train final: (3000, 779)\n Test final: (300, 778)\n\n PRIMERAS FILAS TRAIN (con age y gender):\n   patient_id   age  gender    bmi  hba1c  glucose smoking_status  \\\n0           5  23.0    male  21.05    6.5    200.0        current   \n1          14  70.0  female  32.63    5.5    165.0        unknown   \n2          36  42.0  female  31.50    5.8    200.0          never   \n3          67  71.0    male  39.03    6.3      NaN          never   \n4         127  66.0  female  23.58    5.8    145.0          never   \n\n   has_diabetes  note_count  \n0             0           1  \n1             0           1  \n2             0           1  \n3             1           1  \n4             1           1  \n","output_type":"stream"}],"execution_count":63},{"cell_type":"markdown","source":"## 5Ô∏è‚É£ An√°lisis Exploratorio (EDA Completo)","metadata":{}},{"cell_type":"code","source":"print(f\"\\n EDAD (Age):\")\nprint(f\"   Media: {df_train_final['age'].mean():.1f} a√±os\")\nprint(f\"   Mediana: {df_train_final['age'].median():.1f} a√±os\")\nprint(f\"   Rango: {df_train_final['age'].min():.0f} - {df_train_final['age'].max():.0f} a√±os\")\nprint(f\"   Faltantes: {df_train_final['age'].isna().sum()}\")\n\nprint(f\"\\n G√âNERO (Gender):\")\ngen_dist = df_train_final['gender'].value_counts()\nfor g, c in gen_dist.items():\n    print(f\"   {g}: {c} ({c/len(df_train_final)*100:.1f}%)\")\n\nprint(f\"\\n BMI:\")\nprint(f\"   Media: {df_train_final['bmi'].mean():.2f} ¬± {df_train_final['bmi'].std():.2f}\")\nprint(f\"   Rango: {df_train_final['bmi'].min():.2f} - {df_train_final['bmi'].max():.2f}\")\nprint(f\"   Faltantes: {df_train_final['bmi'].isna().sum()}\")\n\nprint(f\"\\n HbA1c:\")\nprint(f\"   Media: {df_train_final['hba1c'].mean():.2f} ¬± {df_train_final['hba1c'].std():.2f}\")\nprint(f\"   Rango: {df_train_final['hba1c'].min():.2f} - {df_train_final['hba1c'].max():.2f}\")\nprint(f\"   Faltantes: {df_train_final['hba1c'].isna().sum()}\")\n\nprint(f\"\\n GLUCOSA (Glucose):\")\nprint(f\"   Media: {df_train_final['glucose'].mean():.2f} ¬± {df_train_final['glucose'].std():.2f}\")\nprint(f\"   Rango: {df_train_final['glucose'].min():.2f} - {df_train_final['glucose'].max():.2f}\")\nprint(f\"   Faltantes: {df_train_final['glucose'].isna().sum()}\")\n\nprint(f\"\\n HIPERTENSI√ìN:\")\nhyp_count = df_train_final['has_hypertension'].sum()\nprint(f\"   Con hipertensi√≥n: {hyp_count} ({hyp_count/len(df_train_final)*100:.1f}%)\")\n\nprint(f\"\\n ENFERMEDAD CARD√çACA:\")\nhd_count = df_train_final['has_heart_disease'].sum()\nprint(f\"   Con cardiopat√≠a: {hd_count} ({hd_count/len(df_train_final)*100:.1f}%)\")\n\nprint(f\"\\n FUMACI√ìN:\")\nsmoke_dist = df_train_final['smoking_status'].value_counts()\nfor s, c in smoke_dist.items():\n    print(f\"   {s}: {c} ({c/len(df_train_final)*100:.1f}%)\")\n\nprint(f\"\\n DIABETES (TARGET):\")\ndiab_dist = df_train_final['has_diabetes'].value_counts()\nprint(f\"   Negativo (0): {diab_dist[0]} ({diab_dist[0]/len(df_train_final)*100:.1f}%)\")\nprint(f\"   Positivo (1): {diab_dist[1]} ({diab_dist[1]/len(df_train_final)*100:.1f}%)\")\n\nprint(f\"\\n CORRELACI√ìN CON DIABETES:\")\nnumeric_cols = ['age', 'bmi', 'hba1c', 'glucose', 'has_hypertension', 'has_heart_disease']\ncorr = df_train_final[numeric_cols + ['has_diabetes']].corr()['has_diabetes'].sort_values(ascending=False)\nprint(corr.head(8))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:18:44.236993Z","iopub.execute_input":"2025-11-04T16:18:44.237822Z","iopub.status.idle":"2025-11-04T16:18:44.255460Z","shell.execute_reply.started":"2025-11-04T16:18:44.237800Z","shell.execute_reply":"2025-11-04T16:18:44.254387Z"}},"outputs":[{"name":"stdout","text":"\n EDAD (Age):\n   Media: 46.3 a√±os\n   Mediana: 49.0 a√±os\n   Rango: 1 - 80 a√±os\n   Faltantes: 9\n\n G√âNERO (Gender):\n   female: 1663 (55.4%)\n   male: 1337 (44.6%)\n\n BMI:\n   Media: 28.05 ¬± 7.77\n   Rango: 0.00 - 72.21\n   Faltantes: 5\n\n HbA1c:\n   Media: 6.25 ¬± 1.07\n   Rango: 3.50 - 9.00\n   Faltantes: 115\n\n GLUCOSA (Glucose):\n   Media: 161.97 ¬± 43.13\n   Rango: 15.00 - 300.00\n   Faltantes: 211\n\n HIPERTENSI√ìN:\n   Con hipertensi√≥n: 1017 (33.9%)\n\n ENFERMEDAD CARD√çACA:\n   Con cardiopat√≠a: 2874 (95.8%)\n\n FUMACI√ìN:\n   never: 1566 (52.2%)\n   unknown: 768 (25.6%)\n   past: 552 (18.4%)\n   current: 114 (3.8%)\n\n DIABETES (TARGET):\n   Negativo (0): 2100 (70.0%)\n   Positivo (1): 900 (30.0%)\n\n CORRELACI√ìN CON DIABETES:\nhas_diabetes         1.000000\nhba1c                0.535270\nglucose              0.446588\nage                  0.429397\nbmi                  0.315766\nhas_hypertension     0.199608\nhas_heart_disease   -0.062372\nName: has_diabetes, dtype: float64\n","output_type":"stream"}],"execution_count":64},{"cell_type":"markdown","source":"## 6Ô∏è‚É£ Preparaci√≥n para Modelado","metadata":{}},{"cell_type":"code","source":"scaler_bmi = StandardScaler()\nscaler_hba1c = StandardScaler()\nscaler_glucose = StandardScaler()\nscaler_age = StandardScaler()\n\ndf_train_final[\"bmi\"] = scaler_bmi.fit_transform(df_train_final[[\"bmi\"]])\ndf_train_final[\"hba1c\"] = scaler_hba1c.fit_transform(df_train_final[[\"hba1c\"]])\ndf_train_final[\"glucose\"] = scaler_glucose.fit_transform(df_train_final[[\"glucose\"]])\ndf_train_final[\"age\"] = scaler_age.fit_transform(df_train_final[[\"age\"]])\n\ndf_test_final[\"bmi\"] = scaler_bmi.transform(df_test_final[[\"bmi\"]])\ndf_test_final[\"hba1c\"] = scaler_hba1c.transform(df_test_final[[\"hba1c\"]])\ndf_test_final[\"glucose\"] = scaler_glucose.transform(df_test_final[[\"glucose\"]])\ndf_test_final[\"age\"] = scaler_age.transform(df_test_final[[\"age\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:18:44.256169Z","iopub.execute_input":"2025-11-04T16:18:44.256372Z","iopub.status.idle":"2025-11-04T16:18:44.285138Z","shell.execute_reply.started":"2025-11-04T16:18:44.256355Z","shell.execute_reply":"2025-11-04T16:18:44.284338Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"# Preparar X_train e y_train\nX_train = df_train_final.drop(columns=[\"patient_id\", \"has_diabetes\", \"has_heart_disease\", \"note_count\"])\ny_train = df_train_final[\"has_diabetes\"]\n\n# Preparar X_test\nX_test = df_test_final.drop(columns=[\"patient_id\", \"has_heart_disease\", \"note_count\"])\n\n# Rellenar NaNs\nprint(\"\\n Rellenando valores faltantes...\")\n\nfor col in X_train.columns:\n    if X_train[col].dtype == 'object':\n        # Categ√≥ricos: usar moda\n        mode_val = X_train[col].mode()[0] if not X_train[col].mode().empty else \"unknown\"\n        X_train[col] = X_train[col].fillna(mode_val)\n        X_test[col] = X_test[col].fillna(mode_val)\n    else:\n        # Num√©ricos: usar media\n        mean_val = X_train[col].mean()\n        X_train[col] = X_train[col].fillna(mean_val)\n        X_test[col] = X_test[col].fillna(mean_val)\n\n# One-hot encoding para categor√≠as\nprint(\"\\n One-hot encoding para gender y smoking_status...\")\nX_train = pd.get_dummies(X_train, columns=['gender', 'smoking_status'], drop_first=True)\nX_test = pd.get_dummies(X_test, columns=['gender', 'smoking_status'], drop_first=True)\n\n# Alinear columnas\nfor col in set(X_train.columns) - set(X_test.columns):\n    X_test[col] = 0\nfor col in set(X_test.columns) - set(X_train.columns):\n    X_train[col] = 0\n\nX_train = X_train[sorted(X_train.columns)]\nX_test = X_test[sorted(X_train.columns)]\n\nprint(f\"\\n X_train final shape: {X_train.shape}\")\nprint(f\" X_test final shape: {X_test.shape}\")\nprint(f\"\\n Columnas features: {X_train.columns.tolist()[:15]}... (+{len(X_train.columns)-15})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:18:44.285983Z","iopub.execute_input":"2025-11-04T16:18:44.286333Z","iopub.status.idle":"2025-11-04T16:18:44.659342Z","shell.execute_reply.started":"2025-11-04T16:18:44.286309Z","shell.execute_reply":"2025-11-04T16:18:44.658552Z"}},"outputs":[{"name":"stdout","text":"\n Rellenando valores faltantes...\n\n One-hot encoding para gender y smoking_status...\n\n X_train final shape: (3000, 777)\n X_test final shape: (300, 777)\n\n Columnas features: ['age', 'bmi', 'emb_0', 'emb_1', 'emb_10', 'emb_100', 'emb_101', 'emb_102', 'emb_103', 'emb_104', 'emb_105', 'emb_106', 'emb_107', 'emb_108', 'emb_109']... (+762)\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"print(X_train[[\"bmi\", \"age\", \"glucose\", \"hba1c\"]].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:18:44.661025Z","iopub.execute_input":"2025-11-04T16:18:44.661456Z","iopub.status.idle":"2025-11-04T16:18:44.667764Z","shell.execute_reply.started":"2025-11-04T16:18:44.661430Z","shell.execute_reply":"2025-11-04T16:18:44.667098Z"}},"outputs":[{"name":"stdout","text":"        bmi       age       glucose     hba1c\n0 -0.900400 -1.026449  8.818876e-01  0.230055\n1  0.589554  1.041082  7.033485e-02 -0.701727\n2  0.444161 -0.190638  8.818876e-01 -0.422192\n3  1.413017  1.085072 -1.413952e-16  0.043698\n4 -0.574875  0.865122 -3.934096e-01 -0.422192\n","output_type":"stream"}],"execution_count":67},{"cell_type":"markdown","source":"## 7Ô∏è‚É£ Modelado: RandomForest con Validaci√≥n","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\" * 80)\nprint(\"MODELADO: ENSEMBLE CON WEIGHTED SOFT VOTING\")\nprint(\"=\" * 80)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\n\n# Split validaci√≥n\nX_tr, X_val, y_tr, y_val = train_test_split(\n    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n)\n\nprint(f\"\\n Entrenamiento: {X_tr.shape[0]} | Validaci√≥n: {X_val.shape[0]}\")\nprint(f\"   Train pos ratio: {y_tr.mean():.2%}\")\nprint(f\"   Val pos ratio: {y_val.mean():.2%}\")\n\n# ============================================================================\n# Definir modelos base\n# ============================================================================\nprint(\"\\n Entrenando modelos base...\")\n\nclf_rf = RandomForestClassifier(\n    n_estimators=200, max_depth=15, min_samples_split=5, min_samples_leaf=2,\n    class_weight='balanced', n_jobs=-1, random_state=42\n)\n\nclf_lr = LogisticRegression(\n    max_iter=2000, solver='lbfgs', class_weight='balanced', random_state=42\n)\n\nclf_gb = GradientBoostingClassifier(\n    n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42\n)\n\nclf_svc = SVC(\n    kernel='rbf', probability=True, class_weight='balanced', \n    random_state=42, max_iter=2000\n)\n\nbase_models = [\n    ('rf', clf_rf),\n    ('lr', clf_lr),\n    ('gb', clf_gb),\n    ('svc', clf_svc),\n]\n\n# ============================================================================\n# Entrenar base models y calcular m√©tricas en validaci√≥n\n# ============================================================================\nauc_list, f1_list, acc_list, names = [], [], [], []\n\nfor name, model in base_models:\n    print(f\"  {name.upper()} ... \", end=\"\", flush=True)\n    model.fit(X_tr, y_tr)\n\n    y_pred = model.predict(X_val)\n    y_proba = model.predict_proba(X_val)[:, 1]\n\n    auc = roc_auc_score(y_val, y_proba)\n    f1 = f1_score(y_val, y_pred, zero_division=0)\n    acc = accuracy_score(y_val, y_pred)\n\n    auc_list.append(auc)\n    f1_list.append(f1)\n    acc_list.append(acc)\n    names.append(name)\n\n    print(f\"AUC={auc:.4f}, F1={f1:.4f}, Acc={acc:.4f}\")\n\n# ============================================================================\n# Calcular pesos a partir de AUC (normalizado)\n# ============================================================================\nprint(\"\\n Calculando pesos a partir de AUC (normalizado)...\")\n\nauc_clipped = np.clip(auc_list, 0.5, 1.0)  # evita pesos negativos\nraw_weights = (np.array(auc_clipped) - 0.5) + 1e-6\nweights = (raw_weights / raw_weights.sum()).tolist()\n\nprint(\"\\nPesos finales (seg√∫n AUC):\")\nfor n, w, a in zip(names, weights, auc_list):\n    print(f\"  {n.upper():>3s}: w={w:.3f}  (AUC={a:.4f})\")\n\n# ============================================================================\n# VotingClassifier con soft voting ponderado\n# ============================================================================\nprint(\"\\n Creando VotingClassifier con soft voting...\")\n\nvoter = VotingClassifier(\n    estimators=base_models,\n    voting='soft',\n    weights=weights,\n    n_jobs=-1\n)\n\nvoter.fit(X_tr, y_tr)\n\n# Predicciones en validaci√≥n\ny_pred_val = voter.predict(X_val)\ny_pred_proba_val = voter.predict_proba(X_val)[:, 1]\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\" RESULTADOS ENSEMBLE EN VALIDACI√ìN\")\nprint(\"=\" * 80)\n\nacc = accuracy_score(y_val, y_pred_val)\nprec = precision_score(y_val, y_pred_val, zero_division=0)\nrec = recall_score(y_val, y_pred_val, zero_division=0)\nf1 = f1_score(y_val, y_pred_val, zero_division=0)\nauc = roc_auc_score(y_val, y_pred_proba_val) if len(np.unique(y_val)) > 1 else 0\n\nprint(f\"Accuracy:  {acc:.4f}\")\nprint(f\"Precision: {prec:.4f}\")\nprint(f\"Recall:    {rec:.4f}\")\nprint(f\"F1-Score:  {f1:.4f}\")\nprint(f\"ROC-AUC:   {auc:.4f}\")\n\nprint(f\"\\nüìä Matriz de Confusi√≥n:\")\ncm = confusion_matrix(y_val, y_pred_val)\nprint(f\"   TN={cm[0,0]} | FP={cm[0,1]}\")\nprint(f\"   FN={cm[1,0]} | TP={cm[1,1]}\")\n\n# ============================================================================\n# Reentrenar ensemble en TODO el dataset de entrenamiento\n# ============================================================================\nprint(\"\\nüîÑ Reentrenando ensemble en TODO el dataset de entrenamiento...\")\nvoter.fit(X_train, y_train)\nprint(\"‚úÖ Ensemble reentrenado\")\n\n# Guardar modelo para referencia\nimport pickle\nwith open(\"ensemble_model.pkl\", \"wb\") as f:\n    pickle.dump(voter, f)\nprint(\"‚úÖ Modelo guardado: ensemble_model.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:18:44.668465Z","iopub.execute_input":"2025-11-04T16:18:44.668699Z","iopub.status.idle":"2025-11-04T16:23:49.519938Z","shell.execute_reply.started":"2025-11-04T16:18:44.668682Z","shell.execute_reply":"2025-11-04T16:23:49.519104Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nMODELADO: ENSEMBLE CON WEIGHTED SOFT VOTING\n================================================================================\n\n Entrenamiento: 2400 | Validaci√≥n: 600\n   Train pos ratio: 30.00%\n   Val pos ratio: 30.00%\n\n Entrenando modelos base...\n  RF ... AUC=0.8991, F1=0.6280, Acc=0.8183\n  LR ... AUC=0.9299, F1=0.7617, Acc=0.8467\n  GB ... AUC=0.9176, F1=0.7296, Acc=0.8567\n  SVC ... AUC=0.9276, F1=0.7696, Acc=0.8533\n\n Calculando pesos a partir de AUC (normalizado)...\n\nPesos finales (seg√∫n AUC):\n   RF: w=0.238  (AUC=0.8991)\n   LR: w=0.257  (AUC=0.9299)\n   GB: w=0.249  (AUC=0.9176)\n  SVC: w=0.255  (AUC=0.9276)\n\n Creando VotingClassifier con soft voting...\n\n================================================================================\n RESULTADOS ENSEMBLE EN VALIDACI√ìN\n================================================================================\nAccuracy:  0.8533\nPrecision: 0.7875\nRecall:    0.7000\nF1-Score:  0.7412\nROC-AUC:   0.9306\n\nüìä Matriz de Confusi√≥n:\n   TN=386 | FP=34\n   FN=54 | TP=126\n\nüîÑ Reentrenando ensemble en TODO el dataset de entrenamiento...\n‚úÖ Ensemble reentrenado\n‚úÖ Modelo guardado: ensemble_model.pkl\n","output_type":"stream"}],"execution_count":68},{"cell_type":"markdown","source":"## 8Ô∏è‚É£ Predicciones Finales en Test","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\" * 80)\nprint(\"GENERANDO PREDICCIONES EN TEST\")\nprint(\"=\" * 80)\n\nprint(\"\\nüîÆ Prediciendo...\")\ny_pred_test = rf_model.predict(X_test)\ny_pred_proba_test = rf_model.predict_proba(X_test)[:, 1]\n\n# Crear submission\nsubmission = pd.DataFrame({\n    'patient_id': df_test_final['patient_id'],\n    'has_diabetes': y_pred_test,\n    'probability': y_pred_proba_test\n})\n\nprint(f\"\\n‚úÖ {len(submission)} predicciones generadas\")\n\nprint(f\"\\nüìä Distribuci√≥n predicciones:\")\ndist = submission['has_diabetes'].value_counts()\nprint(f\"   Negativo (0): {dist[0]} ({dist[0]/len(submission)*100:.1f}%)\")\nprint(f\"   Positivo (1): {dist[1]} ({dist[1]/len(submission)*100:.1f}%)\")\n\nprint(f\"\\nüìä Probabilidades:\")\nprint(f\"   Media: {submission['probability'].mean():.4f}\")\nprint(f\"   Min: {submission['probability'].min():.4f}\")\nprint(f\"   Max: {submission['probability'].max():.4f}\")\n\nprint(f\"\\nüìã PRIMERAS 10 PREDICCIONES:\")\nprint(submission.head(10).to_string(index=False))\n\n# Guardar\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(f\"\\n‚úÖ Guardado: submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:23:49.520864Z","iopub.execute_input":"2025-11-04T16:23:49.521142Z","iopub.status.idle":"2025-11-04T16:23:49.543509Z","shell.execute_reply.started":"2025-11-04T16:23:49.521118Z","shell.execute_reply":"2025-11-04T16:23:49.542516Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nGENERANDO PREDICCIONES EN TEST\n================================================================================\n\nüîÆ Prediciendo...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_146/2192623632.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüîÆ Prediciendo...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0my_pred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0my_pred_proba_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \"\"\"\n\u001b[0;32m--> 820\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    600\u001b[0m         Validate X whenever one tries to predict, apply, predict_proba.\"\"\"\n\u001b[1;32m    601\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No support for np.int64 index based sparse matrices\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mvalidated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \"\"\"\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"requires_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    479\u001b[0m                 )\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     def _validate_data(\n","\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- has_heart_disease\n- note_count\n"],"ename":"ValueError","evalue":"The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- has_heart_disease\n- note_count\n","output_type":"error"}],"execution_count":69},{"cell_type":"markdown","source":"## 9Ô∏è‚É£ Guardado de Archivos","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\" * 80)\nprint(\"GUARDANDO DATAFRAMES\")\nprint(\"=\" * 80)\n\n# Parquet (comprimido)\ndf_train_final.to_parquet(\"df_train_final.parquet\", index=False)\ndf_test_final.to_parquet(\"df_test_final.parquet\", index=False)\nprint(f\"\\n‚úÖ Parquet (comprimido):\")\nprint(f\"   df_train_final.parquet\")\nprint(f\"   df_test_final.parquet\")\n\n# CSV (primeras 100 filas, legible)\ndf_train_final.head(100).to_csv(\"df_train_sample.csv\", index=False)\ndf_test_final.head(100).to_csv(\"df_test_sample.csv\", index=False)\nprint(f\"\\n‚úÖ CSV (muestras 100 filas):\")\nprint(f\"   df_train_sample.csv\")\nprint(f\"   df_test_sample.csv\")\n\nprint(f\"\\nüíæ Tama√±o en memoria:\")\nprint(f\"   Train: {df_train_final.memory_usage(deep=True).sum() / 1e6:.2f} MB\")\nprint(f\"   Test: {df_test_final.memory_usage(deep=True).sum() / 1e6:.2f} MB\")\n\nprint(f\"\\n‚úÖ ARCHIVOS GENERADOS:\")\nprint(f\"   1. submission.csv (predicciones finales)\")\nprint(f\"   2. df_train_final.parquet (features train + embeddings)\")\nprint(f\"   3. df_test_final.parquet (features test + embeddings)\")\nprint(f\"   4. df_train_sample.csv (muestra train)\")\nprint(f\"   5. df_test_sample.csv (muestra test)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:23:49.543954Z","iopub.status.idle":"2025-11-04T16:23:49.544164Z","shell.execute_reply.started":"2025-11-04T16:23:49.544062Z","shell.execute_reply":"2025-11-04T16:23:49.544072Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üéâ Resumen Final\n\n### ‚úÖ Pipeline Completado:\n\n1. **Carga**: train.json + test.json\n2. **Extracci√≥n**: edad, g√©nero, BMI, HbA1c, glucosa, hipertensi√≥n, cardiopat√≠a, fumaci√≥n\n3. **BioClinicalBERT**: embeddings 768-dimensionales por nota\n4. **Agrupaci√≥n**: promediado por paciente\n5. **Features**: ~780 columnas (10 cl√≠nicas + 768 embeddings + dummies)\n6. **Modelado**: RandomForest 200 √°rboles con validaci√≥n 80/20\n7. **Predicciones**: submission.csv con probabilidades\n\n### üìä Dataset:\n\n- **Train**: 200 pacientes con etiqueta diabetes (133 neg, 67 pos = 33.5%)\n- **Test**: ~300 pacientes sin etiqueta\n- **Features cl√≠nicos**: edad (a√±os), g√©nero (m/f), BMI (18-80), HbA1c (3-20), glucosa (40-600)\n- **Embeddings**: 768-dim via Bio_ClinicalBERT preentrenado en MIMIC-III\n\n### üéØ Modelos/Algoritmos:\n\n- **RandomForest**: 200 √°rboles, max_depth=15, balanced class weights\n- **Validaci√≥n**: 80/20 train/val, stratified por has_diabetes\n- **M√©tricas**: Accuracy, Precision, Recall, F1-Score, ROC-AUC\n\n### üíæ Salidas:\n\n- `submission.csv`: patient_id + has_diabetes (0/1) + probability\n- `df_train_final.parquet`: 200 √ó 776 (patient_id + 9 features + 768 embeddings)\n- `df_test_final.parquet`: 300 √ó 777 (paciente_id + 9 features + 768 embeddings)\n\n### üìñ Pr√≥ximas Mejoras:\n\n- XGBoost o LightGBM (suelen superar RF)\n- Hyperparameter tuning (GridSearchCV/Optuna)\n- Ensemble (combinar RF + XGB + Neural Network)\n- Feature engineering (interacciones, ratios)\n- Neural Networks (embeddings directos + dense layers)\n\n### üîó Cargar datos despu√©s sin re-procesar:\n\n```python\nimport pandas as pd\ndf_train = pd.read_parquet(\"df_train_final.parquet\")\ndf_test = pd.read_parquet(\"df_test_final.parquet\")\nsubmission = pd.read_csv(\"submission.csv\")\n```\n\n---\n\n**Creado**: 03-11-2025  \n**Versi√≥n**: 1.0 - Notebook Completo y Funcional\n","metadata":{}}]}