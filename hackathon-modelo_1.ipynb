{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè• BioClinicalBERT + Machine Learning para Predicci√≥n de Diabetes\n",
    "## Hackathon HackUPM 2025 - Notebook Completo y Funcional\n",
    "\n",
    "**Pipeline completo:**\n",
    "1. Carga: train.json + test.json\n",
    "2. Extracci√≥n: edad, g√©nero, BMI, HbA1c, glucosa, hipertensi√≥n, cardiopat√≠a, fumaci√≥n\n",
    "3. BioClinicalBERT: embeddings 768-dimensionales por nota\n",
    "4. Agrupaci√≥n: promediado por paciente\n",
    "5. Features: ~780 columnas (10 cl√≠nicas + 768 embeddings + dummies)\n",
    "6. Pre-Modelado y Normalizaci√≥n: Generaci√≥n de dataframes para modelar y normalizar las variables \n",
    "7. Modelado: RandomForest 200 √°rboles con validaci√≥n 80/20\n",
    "8. Predicciones: Predicci√≥n del modelo sobre el dataset de test\n",
    "9. Exportaci√≥n: submission.csv con probabilidades\n",
    "\n",
    "**Autor:** Pipeline integrado de IA  \n",
    "**Fecha:** 03-11-2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:43:01.107489Z",
     "iopub.status.busy": "2025-11-04T18:43:01.107277Z",
     "iopub.status.idle": "2025-11-04T18:43:01.111057Z",
     "shell.execute_reply": "2025-11-04T18:43:01.110333Z",
     "shell.execute_reply.started": "2025-11-04T18:43:01.107474Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install word2number\n",
    "# !pip install --upgrade git+https://github.com/huggingface/transformers.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:43:01.112154Z",
     "iopub.status.busy": "2025-11-04T18:43:01.111915Z",
     "iopub.status.idle": "2025-11-04T18:43:01.125638Z",
     "shell.execute_reply": "2025-11-04T18:43:01.125019Z",
     "shell.execute_reply.started": "2025-11-04T18:43:01.112131Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librer√≠as importadas\n",
      "CUDA disponible: True\n",
      "Dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "# INSTALACIONES (ejecutar si es primera vez, descomentar)\n",
    "# !pip install -q transformers torch pandas numpy tqdm scikit-learn\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "from word2number import w2n\n",
    "import warnings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Librer√≠as importadas\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Cargar Datos (train.json + test.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:43:01.126514Z",
     "iopub.status.busy": "2025-11-04T18:43:01.126307Z",
     "iopub.status.idle": "2025-11-04T18:43:01.167873Z",
     "shell.execute_reply": "2025-11-04T18:43:01.167202Z",
     "shell.execute_reply.started": "2025-11-04T18:43:01.126499Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CARGANDO DATOS\n",
      "================================================================================\n",
      "\n",
      "Leyendo train.json...\n",
      "Leyendo test.json...\n",
      "\n",
      "Train: 3000 registros, 3 columnas\n",
      "Test: 300 registros, 2 columnas\n",
      "\n",
      "Distribuci√≥n diabetes en TRAIN:\n",
      "has_diabetes\n",
      "0    2100\n",
      "1     900\n",
      "Name: count, dtype: int64\n",
      "Proporci√≥n positivos: 30.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CARGANDO DATOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nLeyendo train.json...\")\n",
    "with open(\"/kaggle/input/hackathon-dataset/train.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "print(\"Leyendo test.json...\")\n",
    "with open(\"/kaggle/input/hackathon-dataset/test.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Crear DataFrames iniciales\n",
    "df_train_raw = pd.DataFrame(train_data)\n",
    "df_test_raw = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"\\nTrain: {len(df_train_raw)} registros, {df_train_raw.shape[1]} columnas\")\n",
    "print(f\"Test: {len(df_test_raw)} registros, {df_test_raw.shape[1]} columnas\")\n",
    "print(f\"\\nDistribuci√≥n diabetes en TRAIN:\")\n",
    "print(df_train_raw[\"has_diabetes\"].value_counts())\n",
    "print(f\"Proporci√≥n positivos: {df_train_raw['has_diabetes'].mean():.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Funciones de Extracci√≥n (Robustas con Negaciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:43:01.168909Z",
     "iopub.status.busy": "2025-11-04T18:43:01.168670Z",
     "iopub.status.idle": "2025-11-04T18:43:01.187455Z",
     "shell.execute_reply": "2025-11-04T18:43:01.186667Z",
     "shell.execute_reply.started": "2025-11-04T18:43:01.168883Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  [0] age=16, gender=female, bmi=21.5, hba1c=6.2, glucose=140, hyp=0, hd=1, smoking=never\n",
      "\n",
      "  [1] age=15, gender=female, bmi=33.6, hba1c=5.5, glucose=158, hyp=1, hd=1, smoking=unknown\n",
      "\n",
      "  [2] age=54, gender=male, bmi=21.5, hba1c=5.5, glucose=145, hyp=0, hd=1, smoking=current\n",
      "\n",
      " Extracci√≥n verificada\n"
     ]
    }
   ],
   "source": [
    "def safe_float(x):\n",
    "    \"\"\"Convierte a float de forma segura.\"\"\"\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def extract_age(text):\n",
    "    \"\"\"Extrae edad con rango v√°lido 0-120.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return np.nan\n",
    "    t = text.lower()\n",
    "    # Patr√≥n 1: \"X-year-old\" o \"age X\"\n",
    "    m = re.search(r'(\\d{1,3})\\s*(?:year)?-?\\s*(?:year-old|yr|years?\\s*old)', t)\n",
    "    age = None\n",
    "    if not m:\n",
    "        m = re.search(r'(?:age|aged)\\s*(?:is)?\\s*(\\d{1,3})\\b', t)\n",
    "        if not m:\n",
    "            m = re.search(r\"\\b([a-zA-Z]+(?:[-\\s][a-zA-Z]+)*)-year-old\\b\", t)\n",
    "            if m:\n",
    "                age = w2n.word_to_num(m.group(1))\n",
    "    if not age:\n",
    "        age = int(m.group(1)) if m else np.nan\n",
    "    return age if (not np.isnan(age) and 0 <= age <= 120) else np.nan\n",
    "\n",
    "def extract_gender(text):\n",
    "    \"\"\"Extrae g√©nero (male/female/unknown).\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"unknown\"\n",
    "    t = text.lower()\n",
    "    male_count = len(re.findall(r'\\b(?:male|man|he|his|him|boy)\\b', t))\n",
    "    female_count = len(re.findall(r'\\b(?:female|woman|she|her|girl)\\b', t))\n",
    "\n",
    "    if male_count > 0 and female_count == 0:\n",
    "        return \"male\"\n",
    "    elif female_count > 0 and male_count == 0:\n",
    "        return \"female\"\n",
    "    elif male_count == female_count == 0:\n",
    "        return \"unknown\"\n",
    "    else:\n",
    "        return \"male\" if male_count >= female_count else \"female\"\n",
    "\n",
    "def extract_bmi(text):\n",
    "    \"\"\"Extrae BMI con rango v√°lido 8-80.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return np.nan\n",
    "    t = text.lower()\n",
    "    m = re.search(r'\\b(?:bmi|imc)\\b[^0-9]{0,30}(\\d{1,3}(?:\\.\\d+)?)', t)\n",
    "    if not m:\n",
    "        m = re.search(r'\\b(?:bmi|imc)\\b.{0,26}range.{0,10}(\\d{1,3}(?:[.,]\\d+)?)', t)\n",
    "    v = safe_float(m.group(1)) if m else np.nan\n",
    "    return v if (not np.isnan(v) and 0 <= v <= 80) else np.nan\n",
    "\n",
    "def extract_hba1c(text):\n",
    "    \"\"\"Extrae HbA1c con rango v√°lido 3-20.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return np.nan\n",
    "    t = text.lower()\n",
    "    # Ventana local: hasta 20 chars despu√©s de \"hba1c\"\n",
    "    m = re.search(r'(?:hba1c|a1c)[^0-9]{0,20}(\\d{1,2}(?:\\.\\d+)?)\\s*%?', t)\n",
    "    if not m:\n",
    "        pattern = re.compile(\n",
    "            r'\\b(?:hba1c(?: level)?s?\\b.{0,10}(?:is|are|was|were|within|of)?\\s*(very\\s+high|high|elevated|normal|within normal limits|low)|(very\\s+high|high|elevated|normal|within normal limits|low)\\s+(?:levels\\s+of\\s+)?hba1c)\\b',\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        m = pattern.search(t)\n",
    "        if m:\n",
    "            mapping = {\n",
    "                \"normal\": 5.5,\n",
    "                \"elevated\": 6.3,\n",
    "                \"high\": 6.5,\n",
    "                \"very high\": 8\n",
    "                }\n",
    "            v = mapping[m.group(1) or m.group(2)] if m else np.nan\n",
    "            return v if (not np.isnan(v) and 0 <= v <= 20) else np.nan\n",
    "    \n",
    "    v = safe_float(m.group(1)) if m else np.nan\n",
    "    return v if (not np.isnan(v) and 0 <= v <= 20) else np.nan\n",
    "\n",
    "def extract_glucose(text):\n",
    "    \"\"\"Extrae glucosa (aleatoria o postprandial) con rango v√°lido 40-600.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return np.nan\n",
    "    t = text.lower()\n",
    "    # Preferir \"glucose\" + 2-3 d√≠gitos\n",
    "    m = re.search(r'\\bglucose\\b[^0-9]{0,20}(\\d{2,3})(?:\\s*mg/dl)?', t)\n",
    "    v = safe_float(m.group(1)) if m else np.nan\n",
    "    if not m:\n",
    "        pattern = re.compile(\n",
    "            r'\\b(?:'\n",
    "            r'(?:\\w+\\s+)?glucose(?: (?:level|levels|measurement|measurements|reading|readings))?\\b.{0,10}(?:is|are|was|were|within|of)?\\s*(?P<val>very\\s+high|high|elevated|normal|within normal limits|low|decreased|reduced|abnormal)'\n",
    "            r'|'\n",
    "            r'(?P<val2>very\\s+high|high|elevated|normal|within normal limits|low|decreased|reduced|abnormal)\\s+(?:\\w+\\s+)?glucose(?: (?:level|levels|measurement|measurements|reading|readings))?'\n",
    "            r')\\b',\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        m = pattern.search(t)\n",
    "        if m:\n",
    "            mapping = {\n",
    "                \"low\": 70,\n",
    "                \"normal\": 140,\n",
    "                \"within normal limits\": 140,\n",
    "                \"elevated\": 165,\n",
    "                \"high\": 200,\n",
    "                \"abnormal\": 200,\n",
    "                \"very high\": 250,\n",
    "                }\n",
    "            try:\n",
    "                v = mapping[m.group(1) or m.group(2)] if m else np.nan\n",
    "            except:\n",
    "                print(t)\n",
    "                exit(0)\n",
    "            return v if (not np.isnan(v)) else np.nan\n",
    "    return v if (not np.isnan(v)) else np.nan\n",
    "\n",
    "def extract_flags(text):\n",
    "    \"\"\"Extrae hipertensi√≥n, cardiopat√≠a, fumaci√≥n (respeta negaciones).\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return 0, 0, \"unknown\"\n",
    "\n",
    "    t = text.lower()\n",
    "    NEG_PAT = r'(?:no\\s+|without\\s+|denies\\s+|negative\\s+for\\s+|no\\s+history\\s+of\\s+)'\n",
    "\n",
    "    # Hipertensi√≥n: negaci√≥n > pos\n",
    "    hyp_neg = bool(re.search(NEG_PAT + r'(?:hypertension|high\\s+blood\\s+pressure)', t))\n",
    "    hyp_pos = bool(re.search(r'\\bhypertension\\b|\\bhigh\\s+blood\\s+pressure\\b', t)) and not hyp_neg\n",
    "    has_hypertension = 1 if hyp_pos else 0\n",
    "\n",
    "    # Cardiopat√≠a: negaci√≥n > pos\n",
    "    hd_neg = bool(re.search(NEG_PAT + r'(?:heart\\s+disease|cardiovascular)', t))\n",
    "    hd_pos = bool(re.search(r'\\bheart\\s+disease\\b|\\bcardiovascular', t)) and not hd_neg\n",
    "    has_heart_disease = 1 if hd_pos else 0\n",
    "\n",
    "    # Fumaci√≥n\n",
    "    if re.search(r'\\bnon-smoker\\b|\\bnever\\s+smoked\\b', t):\n",
    "        smoking = \"never\"\n",
    "    elif re.search(r'\\b(?:past|former)\\s+(?:smoker|smoking)\\b', t):\n",
    "        smoking = \"past\"\n",
    "    elif re.search(r'\\bcurrent\\s+smoker\\b|\\bis\\s+a\\s+smoker\\b|\\bsmoker\\b', t):\n",
    "        smoking = \"current\"\n",
    "    else:\n",
    "        smoking = \"unknown\"\n",
    "\n",
    "    return has_hypertension, has_heart_disease, smoking\n",
    "\n",
    "# TEST: verificar extracci√≥n en muestra\n",
    "import numpy as np\n",
    "\n",
    "for i in range(min(3, len(df_train_raw))):\n",
    "    note = df_train_raw[\"medical_note\"].iloc[i]\n",
    "    age = extract_age(note)\n",
    "    gender = extract_gender(note)\n",
    "    bmi = extract_bmi(note)\n",
    "    hba1c = extract_hba1c(note)\n",
    "    glucose = extract_glucose(note)\n",
    "    hyp, hd, smoking = extract_flags(note)\n",
    "\n",
    "    age_s    = \"NaN\" if (age is None or (isinstance(age, float) and np.isnan(age))) else f\"{int(age)}\"\n",
    "    bmi_s    = \"NaN\" if (bmi is None or np.isnan(bmi)) else f\"{bmi:.1f}\"\n",
    "    hba1c_s  = \"NaN\" if (hba1c is None or np.isnan(hba1c)) else f\"{hba1c:.1f}\"\n",
    "    glucose_s= \"NaN\" if (glucose is None or np.isnan(glucose)) else f\"{glucose:.0f}\"\n",
    "\n",
    "    print(\n",
    "        f\"\\n  [{i}] age={age_s}, gender={gender}, bmi={bmi_s}, \"\n",
    "        f\"hba1c={hba1c_s}, glucose={glucose_s}, hyp={hyp}, hd={hd}, smoking={smoking}\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"\\n Extracci√≥n verificada\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:43:01.188549Z",
     "iopub.status.busy": "2025-11-04T18:43:01.188321Z",
     "iopub.status.idle": "2025-11-04T18:43:01.606306Z",
     "shell.execute_reply": "2025-11-04T18:43:01.605579Z",
     "shell.execute_reply.started": "2025-11-04T18:43:01.188524Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train: extrayendo features...\n",
      " Test: extrayendo features...\n",
      "\n",
      " Features extra√≠dos correctamente\n",
      "\n",
      " MUESTRA TRAIN (primeras 3 filas):\n",
      "   patient_id   age  gender    bmi  hba1c  glucose smoking_status  \\\n",
      "0       82555  16.0  female  21.49    6.2    140.0          never   \n",
      "1       92299  15.0  female  33.62    5.5    158.0        unknown   \n",
      "2       18725  54.0    male  21.46    5.5    145.0        current   \n",
      "\n",
      "   has_diabetes  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n"
     ]
    }
   ],
   "source": [
    "# Aplicar a TRAIN\n",
    "print(\"\\n Train: extrayendo features...\")\n",
    "df_train_raw[\"age\"] = df_train_raw[\"medical_note\"].apply(extract_age)\n",
    "df_train_raw[\"gender\"] = df_train_raw[\"medical_note\"].apply(extract_gender)\n",
    "df_train_raw[\"bmi\"] = df_train_raw[\"medical_note\"].apply(extract_bmi)\n",
    "df_train_raw[\"hba1c\"] = df_train_raw[\"medical_note\"].apply(extract_hba1c)\n",
    "df_train_raw[\"glucose\"] = df_train_raw[\"medical_note\"].apply(extract_glucose)\n",
    "\n",
    "tmp_train = df_train_raw[\"medical_note\"].apply(extract_flags)\n",
    "df_train_raw[\"has_hypertension\"] = [t[0] for t in tmp_train]\n",
    "df_train_raw[\"has_heart_disease\"] = [t[1] for t in tmp_train]\n",
    "df_train_raw[\"smoking_status\"] = [t[2] for t in tmp_train]\n",
    "\n",
    "# Aplicar a TEST\n",
    "print(\" Test: extrayendo features...\")\n",
    "df_test_raw[\"age\"] = df_test_raw[\"medical_note\"].apply(extract_age)\n",
    "df_test_raw[\"gender\"] = df_test_raw[\"medical_note\"].apply(extract_gender)\n",
    "df_test_raw[\"bmi\"] = df_test_raw[\"medical_note\"].apply(extract_bmi)\n",
    "df_test_raw[\"hba1c\"] = df_test_raw[\"medical_note\"].apply(extract_hba1c)\n",
    "df_test_raw[\"glucose\"] = df_test_raw[\"medical_note\"].apply(extract_glucose)\n",
    "\n",
    "tmp_test = df_test_raw[\"medical_note\"].apply(extract_flags)\n",
    "df_test_raw[\"has_hypertension\"] = [t[0] for t in tmp_test]\n",
    "df_test_raw[\"has_heart_disease\"] = [t[1] for t in tmp_test]\n",
    "df_test_raw[\"smoking_status\"] = [t[2] for t in tmp_test]\n",
    "\n",
    "print(\"\\n Features extra√≠dos correctamente\")\n",
    "print(f\"\\n MUESTRA TRAIN (primeras 3 filas):\")\n",
    "cols = ['patient_id', 'age', 'gender', 'bmi', 'hba1c', 'glucose', 'smoking_status', 'has_diabetes']\n",
    "print(df_train_raw[cols].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:43:01.607395Z",
     "iopub.status.busy": "2025-11-04T18:43:01.607117Z",
     "iopub.status.idle": "2025-11-04T18:43:01.614547Z",
     "shell.execute_reply": "2025-11-04T18:43:01.613814Z",
     "shell.execute_reply.started": "2025-11-04T18:43:01.607368Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patient_id             0\n",
       "has_diabetes           0\n",
       "medical_note           0\n",
       "age                    9\n",
       "gender                 0\n",
       "bmi                    5\n",
       "hba1c                115\n",
       "glucose              211\n",
       "has_hypertension       0\n",
       "has_heart_disease      0\n",
       "smoking_status         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_raw.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ BioClinicalBERT: Generar Embeddings (768-dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:43:01.615610Z",
     "iopub.status.busy": "2025-11-04T18:43:01.615405Z",
     "iopub.status.idle": "2025-11-04T18:43:36.165099Z",
     "shell.execute_reply": "2025-11-04T18:43:36.164440Z",
     "shell.execute_reply.started": "2025-11-04T18:43:01.615597Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cargando emilyalsentzer/Bio_ClinicalBERT...\n",
      "\n",
      " Generando embeddings TRAIN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:30<00:00, 99.95it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Generando embeddings TEST...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:02<00:00, 100.63it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_bioclinicalbert():\n",
    "    \"\"\"Carga Bio_ClinicalBERT desde HuggingFace.\"\"\"\n",
    "    model_name = \"emilyalsentzer/Bio_ClinicalBERT\" # coge el modelo de HuggingFace\n",
    "    print(f\" Cargando {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name) # convierte el texto en tokens\n",
    "    model = AutoModel.from_pretrained(model_name) # Genera los embeddings desde la red a los token de la secuencia\n",
    "    model.eval() # evalua que se ha pasado de texto a embeddings\n",
    "    model.to(device) # mueve el modelo a GPU (kaggle)\n",
    "    return tokenizer, model\n",
    "\n",
    "def mean_pool(last_hidden_state, attention_mask):\n",
    "    \"\"\"Mean pooling con mask.\"\"\"\n",
    "    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float() # mascara que ve cuales partes de los valores del vector influyeno o no \n",
    "    sum_hidden = (last_hidden_state * mask).sum(dim=1) # quita los insignificantes y sumas todos los buenos\n",
    "    sum_mask = torch.clamp(mask.sum(dim=1), min=1e-9) # clump evita que se divida en la siguiente linea entre 0 \n",
    "    return sum_hidden / sum_mask # calcula la media de todos los tokens entre los de la mascara\n",
    "\n",
    "def embed_text(text, tokenizer, model, max_length=512):\n",
    "    \"\"\"Genera 1 embedding de 768 dims para un texto si esta vacia o no es texto.\"\"\"\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0: \n",
    "        return np.zeros(768, dtype=np.float32) \n",
    "\n",
    "    tokens = tokenizer(text,padding=True,truncation=True,max_length=max_length,return_tensors=\"pt\").to(device) # crea los tokens desde la frase\n",
    "\n",
    "    with torch.no_grad(): # sin gradientes para no entrenar modelo\n",
    "        output = model(**tokens) # crea los embeddings\n",
    "        pooled = mean_pool(output.last_hidden_state, tokens[\"attention_mask\"]) # llama a la funcion de la mascara para ponderar correctamente\n",
    "\n",
    "    return pooled.cpu().numpy()[0].astype(np.float32)\n",
    "\n",
    "# Cargar modelo (una sola vez)\n",
    "tokenizer, model = load_bioclinicalbert()\n",
    "\n",
    "# Generar embeddings TRAIN\n",
    "print(\"\\n Generando embeddings TRAIN...\")\n",
    "train_embeddings = []\n",
    "for note in tqdm(df_train_raw[\"medical_note\"].tolist(), desc=\"Train embeddings\", total=len(df_train_raw)):\n",
    "    emb = embed_text(note, tokenizer, model)\n",
    "    train_embeddings.append(emb)\n",
    "\n",
    "df_train_raw[\"embedding\"] = train_embeddings\n",
    "\n",
    "# Generar embeddings TEST\n",
    "print(\"\\n Generando embeddings TEST...\")\n",
    "test_embeddings = []\n",
    "for note in tqdm(df_test_raw[\"medical_note\"].tolist(), desc=\"Test embeddings\", total=len(df_test_raw)):\n",
    "    emb = embed_text(note, tokenizer, model)\n",
    "    test_embeddings.append(emb)\n",
    "\n",
    "df_test_raw[\"embedding\"] = test_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Agrupar por Paciente + Expandir Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:43:36.165923Z",
     "iopub.status.busy": "2025-11-04T18:43:36.165709Z",
     "iopub.status.idle": "2025-11-04T18:43:37.705216Z",
     "shell.execute_reply": "2025-11-04T18:43:37.704496Z",
     "shell.execute_reply.started": "2025-11-04T18:43:36.165908Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Agrupando TRAIN por patient_id...\n",
      " Train agrupado: 3000 pacientes √∫nicos\n",
      "\n",
      " Agrupando TEST por patient_id...\n",
      " Test agrupado: 300 pacientes √∫nicos\n",
      "\n",
      " Expandiendo embeddings (768 columnas)...\n",
      "\n",
      " Train final: (3000, 779)\n",
      " Test final: (300, 778)\n",
      "\n",
      " PRIMERAS FILAS TRAIN (con age y gender):\n",
      "   patient_id   age  gender    bmi  hba1c  glucose smoking_status  \\\n",
      "0           5  23.0    male  21.05    6.5    200.0        current   \n",
      "1          14  70.0  female  32.63    5.5    165.0        unknown   \n",
      "2          36  42.0  female  31.50    5.8    200.0          never   \n",
      "3          67  71.0    male  39.03    6.3      NaN          never   \n",
      "4         127  66.0  female  23.58    5.8    145.0          never   \n",
      "\n",
      "   has_diabetes  note_count  \n",
      "0             0           1  \n",
      "1             0           1  \n",
      "2             0           1  \n",
      "3             1           1  \n",
      "4             1           1  \n"
     ]
    }
   ],
   "source": [
    "def most_common(series):\n",
    "    \"\"\"Retorna valor m√°s frecuente o 'unknown' del embedding.\"\"\"\n",
    "    s = series.dropna()\n",
    "    return s.mode().iat[0] if not s.mode().empty else \"unknown\"\n",
    "\n",
    "def emb_mean(series):\n",
    "    \"\"\"Promedia embeddings.\"\"\"\n",
    "    stacked = np.vstack(series.values)\n",
    "    return stacked.mean(axis=0)\n",
    "\n",
    "\n",
    "# Agregaci√≥n TRAIN\n",
    "print(\"\\n Agrupando TRAIN por patient_id...\")\n",
    "agg_dict = {\n",
    "    \"medical_note\": \"count\",\n",
    "    \"has_diabetes\": \"first\",\n",
    "    \"age\": \"mean\",\n",
    "    \"gender\": most_common,\n",
    "    \"bmi\": \"mean\",\n",
    "    \"hba1c\": \"mean\",\n",
    "    \"glucose\": \"mean\",\n",
    "    \"has_hypertension\": \"max\",\n",
    "    \"has_heart_disease\": \"max\",\n",
    "    \"smoking_status\": most_common,\n",
    "    \"embedding\": emb_mean\n",
    "}\n",
    "\n",
    "df_train_agg = df_train_raw.groupby(\"patient_id\").agg(agg_dict).reset_index() # por si se repiten pacientes, agruparlos\n",
    "df_train_agg.rename(columns={\"medical_note\": \"note_count\"}, inplace=True)\n",
    "\n",
    "print(f\" Train agrupado: {df_train_agg.shape[0]} pacientes √∫nicos\")\n",
    "\n",
    "# Agregaci√≥n TEST (sin has_diabetes)\n",
    "print(\"\\n Agrupando TEST por patient_id...\")\n",
    "agg_dict_test = {\n",
    "    \"medical_note\": \"count\",\n",
    "    \"age\": \"mean\",\n",
    "    \"gender\": most_common,\n",
    "    \"bmi\": \"mean\",\n",
    "    \"hba1c\": \"mean\",\n",
    "    \"glucose\": \"mean\",\n",
    "    \"has_hypertension\": \"max\",\n",
    "    \"has_heart_disease\": \"max\",\n",
    "    \"smoking_status\": most_common,\n",
    "    \"embedding\": emb_mean\n",
    "}\n",
    "\n",
    "df_test_agg = df_test_raw.groupby(\"patient_id\").agg(agg_dict_test).reset_index()\n",
    "df_test_agg.rename(columns={\"medical_note\": \"note_count\"}, inplace=True)\n",
    "\n",
    "print(f\" Test agrupado: {df_test_agg.shape[0]} pacientes √∫nicos\")\n",
    "\n",
    "# Expandir embeddings en columnas\n",
    "print(\"\\n Expandiendo embeddings (768 columnas)...\")\n",
    "\n",
    "emb_train = np.vstack(df_train_agg[\"embedding\"].values)\n",
    "emb_test = np.vstack(df_test_agg[\"embedding\"].values)\n",
    "\n",
    "# Crea los embeddings (columnas) en el Dataframe\n",
    "emb_cols = [f\"emb_{i}\" for i in range(emb_train.shape[1])]\n",
    "emb_df_train = pd.DataFrame(emb_train, columns=emb_cols)\n",
    "emb_df_test = pd.DataFrame(emb_test, columns=emb_cols)\n",
    "\n",
    "df_train_final = pd.concat([df_train_agg.drop(columns=[\"embedding\"]).reset_index(drop=True), emb_df_train], axis=1)\n",
    "df_test_final = pd.concat([df_test_agg.drop(columns=[\"embedding\"]).reset_index(drop=True), emb_df_test], axis=1)\n",
    "\n",
    "print(f\"\\n Train final: {df_train_final.shape}\")\n",
    "print(f\" Test final: {df_test_final.shape}\")\n",
    "\n",
    "print(f\"\\n PRIMERAS FILAS TRAIN (con age y gender):\")\n",
    "print(df_train_final[['patient_id', 'age', 'gender', 'bmi', 'hba1c', 'glucose', 'smoking_status', 'has_diabetes', 'note_count']].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ An√°lisis Exploratorio (EDA Completo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:43:37.706042Z",
     "iopub.status.busy": "2025-11-04T18:43:37.705868Z",
     "iopub.status.idle": "2025-11-04T18:43:37.722624Z",
     "shell.execute_reply": "2025-11-04T18:43:37.721831Z",
     "shell.execute_reply.started": "2025-11-04T18:43:37.706030Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " EDAD (Age):\n",
      "   Media: 46.3 a√±os\n",
      "   Mediana: 49.0 a√±os\n",
      "   Rango: 1 - 80 a√±os\n",
      "   Faltantes: 9\n",
      "\n",
      " G√âNERO (Gender):\n",
      "   female: 1663 (55.4%)\n",
      "   male: 1337 (44.6%)\n",
      "\n",
      " BMI:\n",
      "   Media: 28.05 ¬± 7.77\n",
      "   Rango: 0.00 - 72.21\n",
      "   Faltantes: 5\n",
      "\n",
      " HbA1c:\n",
      "   Media: 6.25 ¬± 1.07\n",
      "   Rango: 3.50 - 9.00\n",
      "   Faltantes: 115\n",
      "\n",
      " GLUCOSA (Glucose):\n",
      "   Media: 161.97 ¬± 43.13\n",
      "   Rango: 15.00 - 300.00\n",
      "   Faltantes: 211\n",
      "\n",
      " HIPERTENSI√ìN:\n",
      "   Con hipertensi√≥n: 1017 (33.9%)\n",
      "\n",
      " ENFERMEDAD CARD√çACA:\n",
      "   Con cardiopat√≠a: 2874 (95.8%)\n",
      "\n",
      " FUMACI√ìN:\n",
      "   never: 1566 (52.2%)\n",
      "   unknown: 768 (25.6%)\n",
      "   past: 552 (18.4%)\n",
      "   current: 114 (3.8%)\n",
      "\n",
      " DIABETES (TARGET):\n",
      "   Negativo (0): 2100 (70.0%)\n",
      "   Positivo (1): 900 (30.0%)\n",
      "\n",
      " CORRELACI√ìN CON DIABETES:\n",
      "has_diabetes         1.000000\n",
      "hba1c                0.535270\n",
      "glucose              0.446588\n",
      "age                  0.429397\n",
      "bmi                  0.315766\n",
      "has_hypertension     0.199608\n",
      "has_heart_disease   -0.062372\n",
      "Name: has_diabetes, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n EDAD (Age):\")\n",
    "print(f\"   Media: {df_train_final['age'].mean():.1f} a√±os\")\n",
    "print(f\"   Mediana: {df_train_final['age'].median():.1f} a√±os\")\n",
    "print(f\"   Rango: {df_train_final['age'].min():.0f} - {df_train_final['age'].max():.0f} a√±os\")\n",
    "print(f\"   Faltantes: {df_train_final['age'].isna().sum()}\")\n",
    "\n",
    "print(f\"\\n G√âNERO (Gender):\")\n",
    "gen_dist = df_train_final['gender'].value_counts()\n",
    "for g, c in gen_dist.items():\n",
    "    print(f\"   {g}: {c} ({c/len(df_train_final)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n BMI:\")\n",
    "print(f\"   Media: {df_train_final['bmi'].mean():.2f} ¬± {df_train_final['bmi'].std():.2f}\")\n",
    "print(f\"   Rango: {df_train_final['bmi'].min():.2f} - {df_train_final['bmi'].max():.2f}\")\n",
    "print(f\"   Faltantes: {df_train_final['bmi'].isna().sum()}\")\n",
    "\n",
    "print(f\"\\n HbA1c:\")\n",
    "print(f\"   Media: {df_train_final['hba1c'].mean():.2f} ¬± {df_train_final['hba1c'].std():.2f}\")\n",
    "print(f\"   Rango: {df_train_final['hba1c'].min():.2f} - {df_train_final['hba1c'].max():.2f}\")\n",
    "print(f\"   Faltantes: {df_train_final['hba1c'].isna().sum()}\")\n",
    "\n",
    "print(f\"\\n GLUCOSA (Glucose):\")\n",
    "print(f\"   Media: {df_train_final['glucose'].mean():.2f} ¬± {df_train_final['glucose'].std():.2f}\")\n",
    "print(f\"   Rango: {df_train_final['glucose'].min():.2f} - {df_train_final['glucose'].max():.2f}\")\n",
    "print(f\"   Faltantes: {df_train_final['glucose'].isna().sum()}\")\n",
    "\n",
    "print(f\"\\n HIPERTENSI√ìN:\")\n",
    "hyp_count = df_train_final['has_hypertension'].sum()\n",
    "print(f\"   Con hipertensi√≥n: {hyp_count} ({hyp_count/len(df_train_final)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n ENFERMEDAD CARD√çACA:\")\n",
    "hd_count = df_train_final['has_heart_disease'].sum()\n",
    "print(f\"   Con cardiopat√≠a: {hd_count} ({hd_count/len(df_train_final)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n FUMACI√ìN:\")\n",
    "smoke_dist = df_train_final['smoking_status'].value_counts()\n",
    "for s, c in smoke_dist.items():\n",
    "    print(f\"   {s}: {c} ({c/len(df_train_final)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n DIABETES (TARGET):\")\n",
    "diab_dist = df_train_final['has_diabetes'].value_counts()\n",
    "print(f\"   Negativo (0): {diab_dist[0]} ({diab_dist[0]/len(df_train_final)*100:.1f}%)\")\n",
    "print(f\"   Positivo (1): {diab_dist[1]} ({diab_dist[1]/len(df_train_final)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n CORRELACI√ìN CON DIABETES:\")\n",
    "numeric_cols = ['age', 'bmi', 'hba1c', 'glucose', 'has_hypertension', 'has_heart_disease']\n",
    "corr = df_train_final[numeric_cols + ['has_diabetes']].corr()['has_diabetes'].sort_values(ascending=False)\n",
    "print(corr.head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Preparaci√≥n para Modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:43:37.723961Z",
     "iopub.status.busy": "2025-11-04T18:43:37.723622Z",
     "iopub.status.idle": "2025-11-04T18:43:38.099830Z",
     "shell.execute_reply": "2025-11-04T18:43:38.098958Z",
     "shell.execute_reply.started": "2025-11-04T18:43:37.723938Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Rellenando valores faltantes...\n",
      "\n",
      " One-hot encoding para gender y smoking_status...\n",
      "\n",
      " X_train final shape: (3000, 777)\n",
      " X_test final shape: (300, 777)\n",
      "\n",
      " Columnas features: ['age', 'bmi', 'emb_0', 'emb_1', 'emb_10', 'emb_100', 'emb_101', 'emb_102', 'emb_103', 'emb_104', 'emb_105', 'emb_106', 'emb_107', 'emb_108', 'emb_109']... (+762)\n"
     ]
    }
   ],
   "source": [
    "# Preparar X_train e y_train\n",
    "X_train = df_train_final.drop(columns=[\"patient_id\", \"has_diabetes\", \"has_heart_disease\", \"note_count\"])\n",
    "y_train = df_train_final[\"has_diabetes\"]\n",
    "\n",
    "# Preparar X_test\n",
    "X_test = df_test_final.drop(columns=[\"patient_id\", \"has_heart_disease\", \"note_count\"])\n",
    "\n",
    "# Rellenar NaNs\n",
    "print(\"\\n Rellenando valores faltantes...\")\n",
    "\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].dtype == 'object':\n",
    "        # Categ√≥ricos: usar moda\n",
    "        mode_val = X_train[col].mode()[0] if not X_train[col].mode().empty else \"unknown\"\n",
    "        X_train[col] = X_train[col].fillna(mode_val)\n",
    "        X_test[col] = X_test[col].fillna(mode_val)\n",
    "    else:\n",
    "        # Num√©ricos: usar media\n",
    "        mean_val = X_train[col].mean()\n",
    "        X_train[col] = X_train[col].fillna(mean_val)\n",
    "        X_test[col] = X_test[col].fillna(mean_val)\n",
    "\n",
    "# One-hot encoding para categor√≠as\n",
    "print(\"\\n One-hot encoding para gender y smoking_status...\")\n",
    "X_train = pd.get_dummies(X_train, columns=['gender', 'smoking_status'], drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, columns=['gender', 'smoking_status'], drop_first=True)\n",
    "\n",
    "# Alinear columnas\n",
    "for col in set(X_train.columns) - set(X_test.columns):\n",
    "    X_test[col] = 0\n",
    "for col in set(X_test.columns) - set(X_train.columns):\n",
    "    X_train[col] = 0\n",
    "\n",
    "X_train = X_train[sorted(X_train.columns)]\n",
    "X_test = X_test[sorted(X_train.columns)]\n",
    "\n",
    "print(f\"\\n X_train final shape: {X_train.shape}\")\n",
    "print(f\" X_test final shape: {X_test.shape}\")\n",
    "print(f\"\\n Columnas features: {X_train.columns.tolist()[:15]}... (+{len(X_train.columns)-15})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:43:38.101077Z",
     "iopub.status.busy": "2025-11-04T18:43:38.100687Z",
     "iopub.status.idle": "2025-11-04T18:43:38.121814Z",
     "shell.execute_reply": "2025-11-04T18:43:38.121273Z",
     "shell.execute_reply.started": "2025-11-04T18:43:38.101051Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scaler_bmi = StandardScaler()\n",
    "scaler_hba1c = StandardScaler()\n",
    "scaler_glucose = StandardScaler()\n",
    "scaler_age = StandardScaler()\n",
    "\n",
    "df_train_final[\"bmi\"] = scaler_bmi.fit_transform(df_train_final[[\"bmi\"]])\n",
    "df_train_final[\"hba1c\"] = scaler_hba1c.fit_transform(df_train_final[[\"hba1c\"]])\n",
    "df_train_final[\"glucose\"] = scaler_glucose.fit_transform(df_train_final[[\"glucose\"]])\n",
    "df_train_final[\"age\"] = scaler_age.fit_transform(df_train_final[[\"age\"]])\n",
    "\n",
    "df_test_final[\"bmi\"] = scaler_bmi.transform(df_test_final[[\"bmi\"]])\n",
    "df_test_final[\"hba1c\"] = scaler_hba1c.transform(df_test_final[[\"hba1c\"]])\n",
    "df_test_final[\"glucose\"] = scaler_glucose.transform(df_test_final[[\"glucose\"]])\n",
    "df_test_final[\"age\"] = scaler_age.transform(df_test_final[[\"age\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:43:38.124292Z",
     "iopub.status.busy": "2025-11-04T18:43:38.123963Z",
     "iopub.status.idle": "2025-11-04T18:43:38.131188Z",
     "shell.execute_reply": "2025-11-04T18:43:38.130440Z",
     "shell.execute_reply.started": "2025-11-04T18:43:38.124269Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     bmi   age     glucose  hba1c\n",
      "0  21.05  23.0  200.000000    6.5\n",
      "1  32.63  70.0  165.000000    5.5\n",
      "2  31.50  42.0  200.000000    5.8\n",
      "3  39.03  71.0  161.966655    6.3\n",
      "4  23.58  66.0  145.000000    5.8\n"
     ]
    }
   ],
   "source": [
    "print(X_train[[\"bmi\", \"age\", \"glucose\", \"hba1c\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Ensemble con Weighted Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:43:38.132199Z",
     "iopub.status.busy": "2025-11-04T18:43:38.131971Z",
     "iopub.status.idle": "2025-11-04T18:48:48.086487Z",
     "shell.execute_reply": "2025-11-04T18:48:48.085642Z",
     "shell.execute_reply.started": "2025-11-04T18:43:38.132184Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODELADO: ENSEMBLE CON WEIGHTED SOFT VOTING\n",
      "================================================================================\n",
      "\n",
      " Entrenamiento: 2400 | Validaci√≥n: 600\n",
      "   Train pos ratio: 30.00%\n",
      "   Val pos ratio: 30.00%\n",
      "\n",
      " Entrenando modelos base...\n",
      "  LR ... AUC=0.9299, F1=0.7668, Acc=0.8500\n",
      "  GB ... AUC=0.9176, F1=0.7296, Acc=0.8567\n",
      "  SVC ... AUC=0.8607, F1=0.6744, Acc=0.7667\n",
      "\n",
      " Calculando pesos a partir de AUC (normalizado)...\n",
      "\n",
      "Pesos finales (seg√∫n AUC):\n",
      "   LR: w=0.356  (AUC=0.9299)\n",
      "   GB: w=0.346  (AUC=0.9176)\n",
      "  SVC: w=0.299  (AUC=0.8607)\n",
      "\n",
      " Creando VotingClassifier con soft voting...\n",
      "\n",
      "================================================================================\n",
      " RESULTADOS ENSEMBLE EN VALIDACI√ìN\n",
      "================================================================================\n",
      "Accuracy:  0.8533\n",
      "Precision: 0.7840\n",
      "Recall:    0.7056\n",
      "F1-Score:  0.7427\n",
      "ROC-AUC:   0.9261\n",
      "\n",
      "Matriz de Confusi√≥n:\n",
      "   TN=385 | FP=35\n",
      "   FN=53 | TP=127\n",
      "\n",
      "Reentrenando ensemble en TODO el dataset de entrenamiento...\n",
      "Ensemble reentrenado\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODELADO: ENSEMBLE CON WEIGHTED SOFT VOTING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Split validaci√≥n\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"\\n Entrenamiento: {X_tr.shape[0]} | Validaci√≥n: {X_val.shape[0]}\")\n",
    "print(f\"   Train pos ratio: {y_tr.mean():.2%}\")\n",
    "print(f\"   Val pos ratio: {y_val.mean():.2%}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Definir modelos base\n",
    "# ============================================================================\n",
    "print(\"\\n Entrenando modelos base...\")\n",
    "\n",
    "clf_rf = RandomForestClassifier(\n",
    "    n_estimators=200, max_depth=15, min_samples_split=5, min_samples_leaf=2,\n",
    "    class_weight='balanced', n_jobs=-1, random_state=42\n",
    ")\n",
    "\n",
    "clf_lr = LogisticRegression(\n",
    "    max_iter=2000, solver='lbfgs', class_weight='balanced', random_state=42\n",
    ")\n",
    "\n",
    "clf_gb = GradientBoostingClassifier(\n",
    "    n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42\n",
    ")\n",
    "\n",
    "clf_svc = SVC(\n",
    "    kernel='rbf', probability=True, class_weight='balanced', \n",
    "    random_state=42, max_iter=2000\n",
    ")\n",
    "\n",
    "base_models = [\n",
    "    ('rf', clf_rf),\n",
    "    ('lr', clf_lr),\n",
    "    ('gb', clf_gb),\n",
    "    ('svc', clf_svc),\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# Entrenar base models y calcular m√©tricas en validaci√≥n\n",
    "# ============================================================================\n",
    "auc_list, f1_list, acc_list, names = [], [], [], []\n",
    "\n",
    "for name, model in base_models:\n",
    "    print(f\"  {name.upper()} ... \", end=\"\", flush=True)\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_proba = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    auc = roc_auc_score(y_val, y_proba)\n",
    "    f1 = f1_score(y_val, y_pred, zero_division=0)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    auc_list.append(auc)\n",
    "    f1_list.append(f1)\n",
    "    acc_list.append(acc)\n",
    "    names.append(name)\n",
    "\n",
    "    print(f\"AUC={auc:.4f}, F1={f1:.4f}, Acc={acc:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Calcular pesos a partir de AUC (normalizado)\n",
    "# ============================================================================\n",
    "print(\"\\n Calculando pesos a partir de AUC (normalizado)...\")\n",
    "\n",
    "auc_clipped = np.clip(auc_list, 0.5, 1.0)  # evita pesos negativos\n",
    "raw_weights = (np.array(auc_clipped) - 0.5) + 1e-6\n",
    "weights = (raw_weights / raw_weights.sum()).tolist()\n",
    "\n",
    "print(\"\\nPesos finales (seg√∫n AUC):\")\n",
    "for n, w, a in zip(names, weights, auc_list):\n",
    "    print(f\"  {n.upper():>3s}: w={w:.3f}  (AUC={a:.4f})\")\n",
    "\n",
    "# ============================================================================\n",
    "# VotingClassifier con soft voting ponderado\n",
    "# ============================================================================\n",
    "print(\"\\n Creando VotingClassifier con soft voting...\")\n",
    "\n",
    "voter = VotingClassifier(\n",
    "    estimators=base_models,\n",
    "    voting='soft',\n",
    "    weights=weights,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "voter.fit(X_tr, y_tr)\n",
    "\n",
    "# Predicciones en validaci√≥n\n",
    "y_pred_val = voter.predict(X_val)\n",
    "y_pred_proba_val = voter.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" RESULTADOS ENSEMBLE EN VALIDACI√ìN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "acc = accuracy_score(y_val, y_pred_val)\n",
    "prec = precision_score(y_val, y_pred_val, zero_division=0)\n",
    "rec = recall_score(y_val, y_pred_val, zero_division=0)\n",
    "f1 = f1_score(y_val, y_pred_val, zero_division=0)\n",
    "auc = roc_auc_score(y_val, y_pred_proba_val) if len(np.unique(y_val)) > 1 else 0\n",
    "\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {auc:.4f}\")\n",
    "\n",
    "print(f\"\\nMatriz de Confusi√≥n:\")\n",
    "cm = confusion_matrix(y_val, y_pred_val)\n",
    "print(f\"   TN={cm[0,0]} | FP={cm[0,1]}\")\n",
    "print(f\"   FN={cm[1,0]} | TP={cm[1,1]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Reentrenar ensemble en TODO el dataset de entrenamiento\n",
    "# ============================================================================\n",
    "print(\"\\nReentrenando ensemble en TODO el dataset de entrenamiento...\")\n",
    "voter.fit(X_train, y_train)\n",
    "print(\"Ensemble reentrenado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Predicciones Finales en Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:48:48.087799Z",
     "iopub.status.busy": "2025-11-04T18:48:48.087393Z",
     "iopub.status.idle": "2025-11-04T18:48:48.718103Z",
     "shell.execute_reply": "2025-11-04T18:48:48.717247Z",
     "shell.execute_reply.started": "2025-11-04T18:48:48.087772Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERANDO PREDICCIONES EN TEST\n",
      "================================================================================\n",
      "\n",
      " Prediciendo...\n",
      "\n",
      "‚úÖ 300 predicciones generadas\n",
      "\n",
      "üìä Distribuci√≥n predicciones:\n",
      "   Negativo (0): 219 (73.0%)\n",
      "   Positivo (1): 81 (27.0%)\n",
      "\n",
      "üìä Probabilidades:\n",
      "   Media: 0.2998\n",
      "   Min: 0.0033\n",
      "   Max: 0.9665\n",
      "\n",
      "üìã PRIMERAS 10 PREDICCIONES:\n",
      " patient_id  has_diabetes  probability\n",
      "        139             0     0.065498\n",
      "        252             0     0.051577\n",
      "        259             0     0.022226\n",
      "        335             1     0.800489\n",
      "        699             0     0.007370\n",
      "        977             0     0.103989\n",
      "       1025             0     0.213758\n",
      "       1145             0     0.117847\n",
      "       1217             0     0.412149\n",
      "       1235             1     0.845419\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERANDO PREDICCIONES EN TEST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n Prediciendo...\")\n",
    "y_pred_test = voter.predict(X_test)\n",
    "y_pred_proba_test = voter.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Crear submission\n",
    "submission = pd.DataFrame({\n",
    "    'patient_id': df_test_final['patient_id'],\n",
    "    'has_diabetes': y_pred_test,\n",
    "    'probability': y_pred_proba_test\n",
    "})\n",
    "\n",
    "print(f\"\\n{len(submission)} predicciones generadas\")\n",
    "\n",
    "print(f\"\\nDistribuci√≥n predicciones:\")\n",
    "dist = submission['has_diabetes'].value_counts()\n",
    "print(f\"   Negativo (0): {dist[0]} ({dist[0]/len(submission)*100:.1f}%)\")\n",
    "print(f\"   Positivo (1): {dist[1]} ({dist[1]/len(submission)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nProbabilidades:\")\n",
    "print(f\"   Media: {submission['probability'].mean():.4f}\")\n",
    "print(f\"   Min: {submission['probability'].min():.4f}\")\n",
    "print(f\"   Max: {submission['probability'].max():.4f}\")\n",
    "\n",
    "print(f\"\\nPRIMERAS 10 PREDICCIONES:\")\n",
    "print(submission.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Guardado de Archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:48:48.719365Z",
     "iopub.status.busy": "2025-11-04T18:48:48.718979Z",
     "iopub.status.idle": "2025-11-04T18:48:48.728243Z",
     "shell.execute_reply": "2025-11-04T18:48:48.727460Z",
     "shell.execute_reply.started": "2025-11-04T18:48:48.719344Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardado: submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Guardar\n",
    "def parse_submission(id):\n",
    "    id = str(id)\n",
    "    length = len(id)\n",
    "    return \"patient_\" + \"0\"* (5 - length) + id\n",
    "\n",
    "submission[\"patient_id\"] = submission[\"patient_id\"].apply(parse_submission)\n",
    "submission[[\"patient_id\",\"has_diabetes\"]].to_csv(\"submission.csv\", index=False)\n",
    "print(f\"\\nGuardado: submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:50:02.875994Z",
     "iopub.status.busy": "2025-11-04T18:50:02.875454Z",
     "iopub.status.idle": "2025-11-04T18:50:09.278695Z",
     "shell.execute_reply": "2025-11-04T18:50:09.277918Z",
     "shell.execute_reply.started": "2025-11-04T18:50:02.875966Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.0->scikit-learn) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.0->scikit-learn) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.0->scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.22.0->scikit-learn) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.22.0->scikit-learn) (2024.2.0)\n",
      "Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\n",
      "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 5.0.0.dev0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed scikit-learn-1.7.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:50:49.759596Z",
     "iopub.status.busy": "2025-11-04T18:50:49.758740Z",
     "iopub.status.idle": "2025-11-04T18:50:49.816633Z",
     "shell.execute_reply": "2025-11-04T18:50:49.815949Z",
     "shell.execute_reply.started": "2025-11-04T18:50:49.759564Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scalers.pkl']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "scalers = [scaler_age, scaler_bmi, scaler_hba1c, scaler_glucose]\n",
    "\n",
    "joblib.dump(voter, \"model.pkl\")\n",
    "joblib.dump(scalers, \"scalers.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen Final\n",
    "\n",
    "### Pipeline Completado:\n",
    "\n",
    "1. **Carga**: train.json + test.json\n",
    "2. **Extracci√≥n**: edad, g√©nero, BMI, HbA1c, glucosa, hipertensi√≥n, cardiopat√≠a, fumaci√≥n\n",
    "3. **BioClinicalBERT**: embeddings 768-dimensionales por nota\n",
    "4. **Agrupaci√≥n**: promediado por paciente\n",
    "5. **Features**: ~780 columnas (10 cl√≠nicas + 768 embeddings + dummies)\n",
    "6. **Pre-Modelado y Normalizaci√≥n**: Generaci√≥n de dataframes para modelar y normalizar las variables \n",
    "7. **Modelado**: RandomForest 200 √°rboles con validaci√≥n 80/20\n",
    "8. **Predicciones**: Predicci√≥n del modelo sobre el dataset de test\n",
    "9. **Exportaci√≥n**: submission.csv con probabilidades\n",
    "\n",
    "### Dataset:\n",
    "\n",
    "- **Features cl√≠nicos**: edad (a√±os), g√©nero (m/f), BMI, HbA1c, glucosa \n",
    "- **Embeddings**: 768-dim via Bio_ClinicalBERT preentrenado en MIMIC-III\n",
    "\n",
    "### Modelos/Algoritmos:\n",
    "\n",
    "- **RandomForest**\n",
    "- **Validaci√≥n**: 80/20 train/val\n",
    "- **M√©tricas**: Accuracy, Precision, Recall, F1-Score, ROC-AUC\n",
    "\n",
    "### Salidas:\n",
    "\n",
    "- `submission.csv`: patient_id + has_diabetes (0/1)\n",
    "\n",
    "---\n",
    "\n",
    "**Creado**: 03-11-2025  \n",
    "**Versi√≥n**: 1.0 - Notebook Completo y Funcional\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8642787,
     "sourceId": 13601163,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
