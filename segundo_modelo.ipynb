{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13601148,"sourceType":"datasetVersion","datasetId":8642778},{"sourceId":13614630,"sourceType":"datasetVersion","datasetId":8652319}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predicción de Diabetes\n## Hackathon HackUPM 2025 \n\n**Pipeline completo:**\n1. Cargar train.json + test.json\n2. Extraer edad, género, features clínicos (con regex robustas y negaciones)\n3. Generar embeddings con BioClinicalBERT (768-dim)\n4. Agrupar por paciente (media de features + embeddings)\n5. EDA completo sin errores\n6. Modelado con RandomForest\n7. Predicciones finales en test\n8. Guardado en múltiples formatos\n\n**Autores:**\n**Fecha:** 03-11-2025\n","metadata":{}},{"cell_type":"code","source":"# INSTALACIONES (ejecutar si es primera vez, descomentar)\n!pip install -q transformers torch pandas numpy tqdm scikit-learn\n!pip install --upgrade git+https://github.com/huggingface/transformers.git\n!pip install word2number\n\nfrom word2number import w2n\nimport json\nimport pandas as pd\nimport numpy as np\nimport torch\nimport re\nimport warnings\nfrom transformers import AutoTokenizer, AutoModel\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n\nwarnings.filterwarnings('ignore')\n\nprint(\"Librerías importadas\")\nprint(f\"CUDA disponible: {torch.cuda.is_available()}\") # CUDA es una plataforma de computación de Nvidia que ejecuta opers en las GPUs.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # prepara tu código para trabajar en la GPU si está disponible, o en la CPU si no lo está.\nprint(f\"Dispositivo: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T18:37:37.128757Z","iopub.execute_input":"2025-11-04T18:37:37.129088Z","iopub.status.idle":"2025-11-04T18:38:04.604453Z","shell.execute_reply.started":"2025-11-04T18:37:37.129066Z","shell.execute_reply":"2025-11-04T18:38:04.603551Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting git+https://github.com/huggingface/transformers.git\n  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-_t2f97hh\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-_t2f97hh\n  Resolved https://github.com/huggingface/transformers.git to commit 020e713ac8e70bd2e72bcd12dc6bd1ada6162562\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (3.19.1)\nRequirement already satisfied: huggingface-hub==1.0.0.rc6 in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (1.0.0rc6)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (2.32.5)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (0.22.1)\nRequirement already satisfied: typer-slim in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (0.19.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==5.0.0.dev0) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (2025.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (0.28.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==5.0.0.dev0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==5.0.0.dev0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==5.0.0.dev0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==5.0.0.dev0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==5.0.0.dev0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==5.0.0.dev0) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==5.0.0.dev0) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==5.0.0.dev0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==5.0.0.dev0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==5.0.0.dev0) (2025.8.3)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer-slim->transformers==5.0.0.dev0) (8.3.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (0.16.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==5.0.0.dev0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==5.0.0.dev0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==5.0.0.dev0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==5.0.0.dev0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==5.0.0.dev0) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub==1.0.0.rc6->transformers==5.0.0.dev0) (1.3.1)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: word2number in /usr/local/lib/python3.11/dist-packages (1.1)\nLibrerías importadas\nCUDA disponible: True\nDispositivo: cuda\n","output_type":"stream"}],"execution_count":52},{"cell_type":"markdown","source":"## 1️⃣ Cargar Datos (train.json + test.json)","metadata":{}},{"cell_type":"code","source":"print(\"\\nLeyendo train.json...\")\nwith open(\"/kaggle/input/fdatas/train1.json\", \"r\") as f:\n    train_data = json.load(f)\n\nprint(\"Leyendo test.json...\")\nwith open(\"/kaggle/input/fdatas/test1.json\", \"r\") as f:\n    test_data = json.load(f)\n\n# Crear DataFrames iniciales\ndf_train_raw = pd.DataFrame(train_data)\ndf_test_raw = pd.DataFrame(test_data)\n\nprint(f\"\\n Train: {len(df_train_raw)} registros, {df_train_raw.shape[1]} columnas\")\nprint(f\" Test: {len(df_test_raw)} registros, {df_test_raw.shape[1]} columnas\")\n\nprint(f\"\\n Columnas : {df_train_raw.columns.tolist()}\")\n\nprint(f\"\\n Distribución diabetes en TRAIN:\")\nprint(df_train_raw[\"has_diabetes\"].value_counts())\nprint(f\"Proporción positivos: {df_train_raw['has_diabetes'].mean()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T18:39:45.089306Z","iopub.execute_input":"2025-11-04T18:39:45.089840Z","iopub.status.idle":"2025-11-04T18:39:45.153520Z","shell.execute_reply.started":"2025-11-04T18:39:45.089817Z","shell.execute_reply":"2025-11-04T18:39:45.152846Z"}},"outputs":[{"name":"stdout","text":"\nLeyendo train.json...\nLeyendo test.json...\n\n Train: 3000 registros, 3 columnas\n Test: 300 registros, 2 columnas\n\n Columnas : ['patient_id', 'has_diabetes', 'medical_note']\n\n Distribución diabetes en TRAIN:\nhas_diabetes\n0    2100\n1     900\nName: count, dtype: int64\nProporción positivos: 0.3\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"## 2️⃣ Extraccion de features clinicos","metadata":{}},{"cell_type":"code","source":"outliers = {\n    \"age\": [],\n    \"glucose\": [],\n    \"hba1c\": [],\n    \"bmi\": []\n}\n\ndef safe_float(x):\n    \"\"\"Convierte a float de forma segura.\"\"\"\n    try:\n        return float(x)\n    except:\n        return np.nan\n\ndef extract_age(text):\n    \"\"\"Extrae edad con rango válido 0-120.\"\"\"\n    if not isinstance(text, str):\n        return np.nan\n    t = text.lower()\n    # Patrón 1: \"X-year-old\" o \"age X\"\n    m = re.search(r'(\\d{1,3})\\s*(?:year)?-?\\s*(?:year-old|yr|years?\\s*old)', t)\n    age = None\n    if not m:\n        m = re.search(r'(?:age|aged)\\s*(?:is)?\\s*(\\d{1,3})\\b', t)\n        if not m:\n            m = re.search(r\"\\b([a-zA-Z]+(?:[-\\s][a-zA-Z]+)*)-year-old\\b\", t)\n            if m:\n                age = w2n.word_to_num(m.group(1))\n    if not age:\n        age = int(m.group(1)) if m else np.nan\n    if not m:\n        outliers[\"age\"].append(t)\n    return age if (not np.isnan(age) and 0 <= age <= 120) else np.nan\n\ndef extract_gender(text):\n    \"\"\"Extrae género (male/female/unknown).\"\"\"\n    if not isinstance(text, str):\n        return \"unknown\"\n    t = text.lower()\n    male_count = len(re.findall(r'\\b(?:male|man|he|his|him|boy)\\b', t))\n    female_count = len(re.findall(r'\\b(?:female|woman|she|her|girl)\\b', t))\n\n    if male_count > 0 and female_count == 0:\n        return \"male\"\n    elif female_count > 0 and male_count == 0:\n        return \"female\"\n    elif male_count == female_count == 0:\n        return \"unknown\"\n    else:\n        return \"male\" if male_count >= female_count else \"female\"\n\ndef extract_bmi(text):\n    \"\"\"Extrae BMI con rango válido 8-80.\"\"\"\n    if not isinstance(text, str):\n        return np.nan\n    t = text.lower()\n    m = re.search(r'\\b(?:bmi|imc)\\b[^0-9]{0,30}(\\d{1,3}(?:\\.\\d+)?)', t)\n    if not m:\n        m = re.search(r'\\b(?:bmi|imc)\\b.{0,26}range.{0,10}(\\d{1,3}(?:[.,]\\d+)?)', t)\n    v = safe_float(m.group(1)) if m else np.nan\n    if not m:\n        outliers[\"bmi\"].append(t)\n    return v if (not np.isnan(v) and 0 <= v <= 80) else np.nan\n\ndef extract_hba1c(text):\n    \"\"\"Extrae HbA1c con rango válido 3-20.\"\"\"\n    if not isinstance(text, str):\n        return np.nan\n    t = text.lower()\n    # Ventana local: hasta 20 chars después de \"hba1c\"\n    m = re.search(r'(?:hba1c|a1c)[^0-9]{0,20}(\\d{1,2}(?:\\.\\d+)?)\\s*%?', t)\n    if not m:\n        pattern = re.compile(\n            r'\\b(?:hba1c(?: level)?s?\\b.{0,10}(?:is|are|was|were|within|of)?\\s*(very\\s+high|high|elevated|normal|within normal limits|low)|(very\\s+high|high|elevated|normal|within normal limits|low)\\s+(?:levels\\s+of\\s+)?hba1c)\\b',\n            re.IGNORECASE\n        )\n        m = pattern.search(t)\n        if m:\n            mapping = {\n                \"normal\": 5.5,\n                \"elevated\": 6.3,\n                \"high\": 6.5,\n                \"very high\": 8\n                }\n            v = mapping[m.group(1) or m.group(2)] if m else np.nan\n            return v if (not np.isnan(v) and 0 <= v <= 20) else np.nan\n    if not m:\n        outliers[\"hba1c\"].append(t)\n    \n    v = safe_float(m.group(1)) if m else np.nan\n    return v if (not np.isnan(v) and 0 <= v <= 20) else np.nan\n\ndef extract_glucose(text):\n    \"\"\"Extrae glucosa (aleatoria o postprandial) con rango válido 40-600.\"\"\"\n    if not isinstance(text, str):\n        return np.nan\n    t = text.lower()\n    # Preferir \"glucose\" + 2-3 dígitos\n    m = re.search(r'\\bglucose\\b[^0-9]{0,20}(\\d{2,3})(?:\\s*mg/dl)?', t)\n    v = safe_float(m.group(1)) if m else np.nan\n    if not m:\n        pattern = re.compile(\n            r'\\b(?:'\n            r'(?:\\w+\\s+)?glucose(?: (?:level|levels|measurement|measurements|reading|readings))?\\b.{0,10}(?:is|are|was|were|within|of)?\\s*(?P<val>very\\s+high|high|elevated|normal|within normal limits|low|decreased|reduced|abnormal)'\n            r'|'\n            r'(?P<val2>very\\s+high|high|elevated|normal|within normal limits|low|decreased|reduced|abnormal)\\s+(?:\\w+\\s+)?glucose(?: (?:level|levels|measurement|measurements|reading|readings))?'\n            r')\\b',\n            re.IGNORECASE\n        )\n        m = pattern.search(t)\n        if m:\n            mapping = {\n                \"low\": 70,\n                \"normal\": 140,\n                \"within normal limits\": 140,\n                \"elevated\": 165,\n                \"high\": 200,\n                \"abnormal\": 200,\n                \"very high\": 250,\n                }\n            try:\n                v = mapping[m.group(1) or m.group(2)] if m else np.nan\n            except:\n                print(t)\n                exit(0)\n            return v if (not np.isnan(v)) else np.nan\n    if not m:\n        outliers[\"glucose\"].append(t)\n    return v if (not np.isnan(v)) else np.nan\n\ndef extract_flags(text):\n    \"\"\"Extrae hipertensión, cardiopatía, fumación (respeta negaciones).\"\"\"\n    if not isinstance(text, str):\n        return 0, 0, \"unknown\"\n\n    t = text.lower()\n    NEG_PAT = r'(?:no\\s+|without\\s+|denies\\s+|negative\\s+for\\s+|no\\s+history\\s+of\\s+)'\n\n    # Hipertensión: negación > pos\n    hyp_neg = bool(re.search(NEG_PAT + r'(?:hypertension|high\\s+blood\\s+pressure)', t))\n    hyp_pos = bool(re.search(r'\\bhypertension\\b|\\bhigh\\s+blood\\s+pressure\\b', t)) and not hyp_neg\n    has_hypertension = 1 if hyp_pos else 0\n\n    # Cardiopatía: negación > pos\n    hd_neg = bool(re.search(NEG_PAT + r'(?:heart\\s+disease|cardiovascular)', t))\n    hd_pos = bool(re.search(r'\\bheart\\s+disease\\b|\\bcardiovascular', t)) and not hd_neg\n    has_heart_disease = 1 if hd_pos else 0\n\n    # Fumación\n    if re.search(r'\\bnon-smoker\\b|\\bnever\\s+smoked\\b', t):\n        smoking = \"never\"\n    elif re.search(r'\\b(?:past|former)\\s+(?:smoker|smoking)\\b', t):\n        smoking = \"past\"\n    elif re.search(r'\\bcurrent\\s+smoker\\b|\\bis\\s+a\\s+smoker\\b|\\bsmoker\\b', t):\n        smoking = \"current\"\n    else:\n        smoking = \"unknown\"\n\n    return has_hypertension, has_heart_disease, smoking\n\n# TEST: verificar extracción en muestra\nimport numpy as np\n\nfor i in range(min(3, len(df_train_raw))):\n    note = df_train_raw[\"medical_note\"].iloc[i]\n    age = extract_age(note)\n    gender = extract_gender(note)\n    bmi = extract_bmi(note)\n    hba1c = extract_hba1c(note)\n    glucose = extract_glucose(note)\n    hyp, hd, smoking = extract_flags(note)\n\n    age_s    = \"NaN\" if (age is None or (isinstance(age, float) and np.isnan(age))) else f\"{int(age)}\"\n    bmi_s    = \"NaN\" if (bmi is None or np.isnan(bmi)) else f\"{bmi:.1f}\"\n    hba1c_s  = \"NaN\" if (hba1c is None or np.isnan(hba1c)) else f\"{hba1c:.1f}\"\n    glucose_s= \"NaN\" if (glucose is None or np.isnan(glucose)) else f\"{glucose:.0f}\"\n\n    print(\n        f\"\\n  [{i}] age={age_s}, gender={gender}, bmi={bmi_s}, \"\n        f\"hba1c={hba1c_s}, glucose={glucose_s}, hyp={hyp}, hd={hd}, smoking={smoking}\"\n    )\n\n\nprint(\"\\n✅ Extracción verificada\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T18:39:50.468431Z","iopub.execute_input":"2025-11-04T18:39:50.468727Z","iopub.status.idle":"2025-11-04T18:39:50.490965Z","shell.execute_reply.started":"2025-11-04T18:39:50.468705Z","shell.execute_reply":"2025-11-04T18:39:50.490240Z"}},"outputs":[{"name":"stdout","text":"\n  [0] age=16, gender=female, bmi=21.5, hba1c=6.2, glucose=140, hyp=0, hd=1, smoking=never\n\n  [1] age=15, gender=female, bmi=33.6, hba1c=5.5, glucose=158, hyp=1, hd=1, smoking=unknown\n\n  [2] age=54, gender=male, bmi=21.5, hba1c=5.5, glucose=145, hyp=0, hd=1, smoking=current\n\n✅ Extracción verificada\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"# Aplicar a TRAIN\nprint(\"\\n Train: extrayendo features...\")\ndf_train_raw[\"age\"] = df_train_raw[\"medical_note\"].apply(extract_age)\ndf_train_raw[\"gender\"] = df_train_raw[\"medical_note\"].apply(extract_gender)\ndf_train_raw[\"bmi\"] = df_train_raw[\"medical_note\"].apply(extract_bmi)\ndf_train_raw[\"hba1c\"] = df_train_raw[\"medical_note\"].apply(extract_hba1c)\ndf_train_raw[\"glucose\"] = df_train_raw[\"medical_note\"].apply(extract_glucose)\n\ntmp_train = df_train_raw[\"medical_note\"].apply(extract_flags)\ndf_train_raw[\"has_hypertension\"] = [t[0] for t in tmp_train]\ndf_train_raw[\"has_heart_disease\"] = [t[1] for t in tmp_train]\ndf_train_raw[\"smoking_status\"] = [t[2] for t in tmp_train]\n\n# Aplicar a TEST\nprint(\" Test: extrayendo features...\")\ndf_test_raw[\"age\"] = df_test_raw[\"medical_note\"].apply(extract_age)\ndf_test_raw[\"gender\"] = df_test_raw[\"medical_note\"].apply(extract_gender)\ndf_test_raw[\"bmi\"] = df_test_raw[\"medical_note\"].apply(extract_bmi)\ndf_test_raw[\"hba1c\"] = df_test_raw[\"medical_note\"].apply(extract_hba1c)\ndf_test_raw[\"glucose\"] = df_test_raw[\"medical_note\"].apply(extract_glucose)\n\ntmp_test = df_test_raw[\"medical_note\"].apply(extract_flags)\ndf_test_raw[\"has_hypertension\"] = [t[0] for t in tmp_test]\ndf_test_raw[\"has_heart_disease\"] = [t[1] for t in tmp_test]\ndf_test_raw[\"smoking_status\"] = [t[2] for t in tmp_test]\n\nprint(\"\\n Features extraídos correctamente\")\nprint(f\"\\n MUESTRA TRAIN (primeras 3 filas):\")\ncols = ['patient_id', 'age', 'gender', 'bmi', 'hba1c', 'glucose', 'smoking_status', 'has_diabetes']\nprint(df_train_raw[cols].head(3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T18:39:57.795584Z","iopub.execute_input":"2025-11-04T18:39:57.795862Z","iopub.status.idle":"2025-11-04T18:39:58.213683Z","shell.execute_reply.started":"2025-11-04T18:39:57.795840Z","shell.execute_reply":"2025-11-04T18:39:58.212898Z"}},"outputs":[{"name":"stdout","text":"\n Train: extrayendo features...\n Test: extrayendo features...\n\n Features extraídos correctamente\n\n MUESTRA TRAIN (primeras 3 filas):\n   patient_id   age  gender    bmi  hba1c  glucose smoking_status  \\\n0       82555  16.0  female  21.49    6.2    140.0          never   \n1       92299  15.0  female  33.62    5.5    158.0        unknown   \n2       18725  54.0    male  21.46    5.5    145.0        current   \n\n   has_diabetes  \n0             0  \n1             0  \n2             0  \n","output_type":"stream"}],"execution_count":56},{"cell_type":"markdown","source":"## 3️⃣ BioClinicalBERT: Generar Embeddings (768-dim)","metadata":{}},{"cell_type":"code","source":"def load_bioclinicalbert():\n    \"\"\"Carga Bio_ClinicalBERT desde HuggingFace.\"\"\"\n    model_name = \"emilyalsentzer/Bio_ClinicalBERT\" # coge el modelo de HuggingFace\n    print(f\" Cargando {model_name}...\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name) # convierte el texto en tokens\n    model = AutoModel.from_pretrained(model_name) # Genera los embeddings desde la red a los token de la secuencia\n    model.eval() # evalua que se ha pasado de texto a embeddings\n    model.to(device) # mueve el modelo a GPU (kaggle)\n    return tokenizer, model\n\ndef mean_pool(last_hidden_state, attention_mask):\n    \"\"\"Mean pooling con mask.\"\"\"\n    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float() # mascara que ve cuales partes de los valores del vector influyeno o no \n    sum_hidden = (last_hidden_state * mask).sum(dim=1) # quita los insignificantes y sumas todos los buenos\n    sum_mask = torch.clamp(mask.sum(dim=1), min=1e-9) # clump evita que se divida en la siguiente linea entre 0 \n    return sum_hidden / sum_mask # calcula la media de todos los tokens entre los de la mascara\n\ndef embed_text(text, tokenizer, model, max_length=512):\n    \"\"\"Genera 1 embedding de 768 dims para un texto si esta vacia o no es texto.\"\"\"\n    if not isinstance(text, str) or len(text.strip()) == 0: \n        return np.zeros(768, dtype=np.float32) \n\n    tokens = tokenizer(text,padding=True,truncation=True,max_length=max_length,return_tensors=\"pt\").to(device) # crea los tokens desde la frase\n\n    with torch.no_grad(): # sin gradientes para no entrenar modelo\n        output = model(**tokens) # crea los embeddings\n        pooled = mean_pool(output.last_hidden_state, tokens[\"attention_mask\"]) # llama a la funcion de la mascara para ponderar correctamente\n\n    return pooled.cpu().numpy()[0].astype(np.float32)\n\n# Cargar modelo (una sola vez)\ntokenizer, model = load_bioclinicalbert()\n\n# Generar embeddings TRAIN\nprint(\"\\n Generando embeddings TRAIN...\")\ntrain_embeddings = []\nfor note in tqdm(df_train_raw[\"medical_note\"].tolist(), desc=\"Train embeddings\", total=len(df_train_raw)):\n    emb = embed_text(note, tokenizer, model)\n    train_embeddings.append(emb)\n\ndf_train_raw[\"embedding\"] = train_embeddings\n\n# Generar embeddings TEST\nprint(\"\\n Generando embeddings TEST...\")\ntest_embeddings = []\nfor note in tqdm(df_test_raw[\"medical_note\"].tolist(), desc=\"Test embeddings\", total=len(df_test_raw)):\n    emb = embed_text(note, tokenizer, model)\n    test_embeddings.append(emb)\n\ndf_test_raw[\"embedding\"] = test_embeddings\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T18:40:01.752558Z","iopub.execute_input":"2025-11-04T18:40:01.752837Z","iopub.status.idle":"2025-11-04T18:40:37.949712Z","shell.execute_reply.started":"2025-11-04T18:40:01.752814Z","shell.execute_reply":"2025-11-04T18:40:37.948913Z"}},"outputs":[{"name":"stdout","text":" Cargando emilyalsentzer/Bio_ClinicalBERT...\n\n Generando embeddings TRAIN...\n","output_type":"stream"},{"name":"stderr","text":"Train embeddings: 100%|██████████| 3000/3000 [00:30<00:00, 98.18it/s] \n","output_type":"stream"},{"name":"stdout","text":"\n Generando embeddings TEST...\n","output_type":"stream"},{"name":"stderr","text":"Test embeddings: 100%|██████████| 300/300 [00:03<00:00, 98.75it/s] \n","output_type":"stream"}],"execution_count":57},{"cell_type":"markdown","source":"## 4️⃣ Agrupar por Paciente + Expandir Embeddings","metadata":{}},{"cell_type":"code","source":"def most_common(series):\n    \"\"\"Retorna valor más frecuente o 'unknown' del embedding.\"\"\"\n    s = series.dropna()\n    return s.mode().iat[0] if not s.mode().empty else \"unknown\"\n\ndef emb_mean(series):\n    \"\"\"Promedia embeddings.\"\"\"\n    stacked = np.vstack(series.values)\n    return stacked.mean(axis=0)\n\n\n# Agregación TRAIN\nprint(\"\\n Agrupando TRAIN por patient_id...\")\nagg_dict = {\n    \"medical_note\": \"count\",\n    \"has_diabetes\": \"first\",\n    \"age\": \"mean\",\n    \"gender\": most_common,\n    \"bmi\": \"mean\",\n    \"hba1c\": \"mean\",\n    \"glucose\": \"mean\",\n    \"has_hypertension\": \"max\",\n    \"has_heart_disease\": \"max\",\n    \"smoking_status\": most_common,\n    \"embedding\": emb_mean\n}\n\ndf_train_agg = df_train_raw.groupby(\"patient_id\").agg(agg_dict).reset_index() # por si se repiten pacientes, agruparlos\ndf_train_agg.rename(columns={\"medical_note\": \"note_count\"}, inplace=True)\n\nprint(f\" Train agrupado: {df_train_agg.shape[0]} pacientes únicos\")\n\n# Agregación TEST (sin has_diabetes)\nprint(\"\\n Agrupando TEST por patient_id...\")\nagg_dict_test = {\n    \"medical_note\": \"count\",\n    \"age\": \"mean\",\n    \"gender\": most_common,\n    \"bmi\": \"mean\",\n    \"hba1c\": \"mean\",\n    \"glucose\": \"mean\",\n    \"has_hypertension\": \"max\",\n    \"has_heart_disease\": \"max\",\n    \"smoking_status\": most_common,\n    \"embedding\": emb_mean\n}\n\ndf_test_agg = df_test_raw.groupby(\"patient_id\").agg(agg_dict_test).reset_index()\ndf_test_agg.rename(columns={\"medical_note\": \"note_count\"}, inplace=True)\n\nprint(f\" Test agrupado: {df_test_agg.shape[0]} pacientes únicos\")\n\n# Expandir embeddings en columnas\nprint(\"\\n Expandiendo embeddings (768 columnas)...\")\n\nemb_train = np.vstack(df_train_agg[\"embedding\"].values)\nemb_test = np.vstack(df_test_agg[\"embedding\"].values)\n\n# Crea los embeddings (columnas) en el Dataframe\nemb_cols = [f\"emb_{i}\" for i in range(emb_train.shape[1])]\nemb_df_train = pd.DataFrame(emb_train, columns=emb_cols)\nemb_df_test = pd.DataFrame(emb_test, columns=emb_cols)\n\ndf_train_final = pd.concat([df_train_agg.drop(columns=[\"embedding\"]).reset_index(drop=True), emb_df_train], axis=1)\ndf_test_final = pd.concat([df_test_agg.drop(columns=[\"embedding\"]).reset_index(drop=True), emb_df_test], axis=1)\n\nprint(f\"\\n Train final: {df_train_final.shape}\")\nprint(f\" Test final: {df_test_final.shape}\")\n\nprint(f\"\\n PRIMERAS FILAS TRAIN (con age y gender):\")\nprint(df_train_final[['patient_id', 'age', 'gender', 'bmi', 'hba1c', 'glucose', 'smoking_status', 'has_diabetes', 'note_count']].head(5))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T18:41:17.482201Z","iopub.execute_input":"2025-11-04T18:41:17.482712Z","iopub.status.idle":"2025-11-04T18:41:19.235676Z","shell.execute_reply.started":"2025-11-04T18:41:17.482689Z","shell.execute_reply":"2025-11-04T18:41:19.234937Z"}},"outputs":[{"name":"stdout","text":"\n Agrupando TRAIN por patient_id...\n Train agrupado: 3000 pacientes únicos\n\n Agrupando TEST por patient_id...\n Test agrupado: 300 pacientes únicos\n\n Expandiendo embeddings (768 columnas)...\n\n Train final: (3000, 779)\n Test final: (300, 778)\n\n PRIMERAS FILAS TRAIN (con age y gender):\n   patient_id   age  gender    bmi  hba1c  glucose smoking_status  \\\n0           5  23.0    male  21.05    6.5    200.0        current   \n1          14  70.0  female  32.63    5.5    165.0        unknown   \n2          36  42.0  female  31.50    5.8    200.0          never   \n3          67  71.0    male  39.03    6.3      NaN          never   \n4         127  66.0  female  23.58    5.8    145.0          never   \n\n   has_diabetes  note_count  \n0             0           1  \n1             0           1  \n2             0           1  \n3             1           1  \n4             1           1  \n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"df_train_final","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T18:41:43.278064Z","iopub.execute_input":"2025-11-04T18:41:43.278569Z","iopub.status.idle":"2025-11-04T18:41:43.300877Z","shell.execute_reply.started":"2025-11-04T18:41:43.278544Z","shell.execute_reply":"2025-11-04T18:41:43.300152Z"}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"      patient_id  note_count  has_diabetes   age  gender    bmi  hba1c  \\\n0              5           1             0  23.0    male  21.05    6.5   \n1             14           1             0  70.0  female  32.63    5.5   \n2             36           1             0  42.0  female  31.50    5.8   \n3             67           1             1  71.0    male  39.03    6.3   \n4            127           1             1  66.0  female  23.58    5.8   \n...          ...         ...           ...   ...     ...    ...    ...   \n2995       95954           1             0  61.0    male  24.52    6.6   \n2996       96030           1             0  73.0  female  25.07    6.3   \n2997       96035           1             0  18.0    male  37.65    6.5   \n2998       96093           1             1  74.0  female   6.00    6.8   \n2999       96095           1             0  64.0  female  25.48    6.5   \n\n      glucose  has_hypertension  has_heart_disease  ...   emb_758   emb_759  \\\n0       200.0                 0                  1  ...  0.168776 -0.137289   \n1       165.0                 0                  1  ...  0.140446 -0.112927   \n2       200.0                 0                  1  ...  0.144146 -0.082072   \n3         NaN                 0                  1  ...  0.158193 -0.121447   \n4       145.0                 0                  1  ...  0.054226 -0.093798   \n...       ...               ...                ...  ...       ...       ...   \n2995    165.0                 0                  1  ...  0.172905 -0.028232   \n2996    158.0                 0                  1  ...  0.050674 -0.060000   \n2997     90.0                 0                  1  ...  0.141482 -0.078641   \n2998    165.0                 1                  0  ...  0.124343  0.006571   \n2999    140.0                 0                  1  ...  0.085310 -0.102265   \n\n       emb_760   emb_761   emb_762   emb_763   emb_764   emb_765   emb_766  \\\n0    -0.300718  0.055569 -0.012067 -0.191791  0.101528  0.040025 -0.036748   \n1    -0.244349 -0.033866  0.026291 -0.076418  0.078653  0.214932 -0.077384   \n2    -0.269926 -0.027545  0.041318 -0.097104  0.003274  0.072266 -0.098089   \n3    -0.211743 -0.014441 -0.079715 -0.143533  0.071336  0.013479 -0.085260   \n4    -0.332355 -0.064596  0.043048 -0.200588  0.136516  0.041142 -0.021674   \n...        ...       ...       ...       ...       ...       ...       ...   \n2995 -0.289735  0.031709 -0.036546 -0.199922  0.011492  0.098776 -0.031317   \n2996 -0.283743 -0.085756 -0.056065 -0.177056  0.061681  0.067599 -0.042998   \n2997 -0.259597  0.134808  0.033681 -0.146596  0.002997  0.081718  0.014971   \n2998 -0.255402 -0.014538 -0.036401 -0.155102  0.129003  0.092416 -0.071230   \n2999 -0.257324 -0.022427  0.071746 -0.198685  0.103298  0.043273 -0.056627   \n\n       emb_767  \n0    -0.125205  \n1     0.030703  \n2    -0.058813  \n3     0.011425  \n4    -0.084981  \n...        ...  \n2995 -0.044507  \n2996 -0.052660  \n2997 -0.069236  \n2998 -0.012100  \n2999 -0.058333  \n\n[3000 rows x 779 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>patient_id</th>\n      <th>note_count</th>\n      <th>has_diabetes</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>bmi</th>\n      <th>hba1c</th>\n      <th>glucose</th>\n      <th>has_hypertension</th>\n      <th>has_heart_disease</th>\n      <th>...</th>\n      <th>emb_758</th>\n      <th>emb_759</th>\n      <th>emb_760</th>\n      <th>emb_761</th>\n      <th>emb_762</th>\n      <th>emb_763</th>\n      <th>emb_764</th>\n      <th>emb_765</th>\n      <th>emb_766</th>\n      <th>emb_767</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>23.0</td>\n      <td>male</td>\n      <td>21.05</td>\n      <td>6.5</td>\n      <td>200.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.168776</td>\n      <td>-0.137289</td>\n      <td>-0.300718</td>\n      <td>0.055569</td>\n      <td>-0.012067</td>\n      <td>-0.191791</td>\n      <td>0.101528</td>\n      <td>0.040025</td>\n      <td>-0.036748</td>\n      <td>-0.125205</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>14</td>\n      <td>1</td>\n      <td>0</td>\n      <td>70.0</td>\n      <td>female</td>\n      <td>32.63</td>\n      <td>5.5</td>\n      <td>165.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.140446</td>\n      <td>-0.112927</td>\n      <td>-0.244349</td>\n      <td>-0.033866</td>\n      <td>0.026291</td>\n      <td>-0.076418</td>\n      <td>0.078653</td>\n      <td>0.214932</td>\n      <td>-0.077384</td>\n      <td>0.030703</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>36</td>\n      <td>1</td>\n      <td>0</td>\n      <td>42.0</td>\n      <td>female</td>\n      <td>31.50</td>\n      <td>5.8</td>\n      <td>200.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.144146</td>\n      <td>-0.082072</td>\n      <td>-0.269926</td>\n      <td>-0.027545</td>\n      <td>0.041318</td>\n      <td>-0.097104</td>\n      <td>0.003274</td>\n      <td>0.072266</td>\n      <td>-0.098089</td>\n      <td>-0.058813</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>67</td>\n      <td>1</td>\n      <td>1</td>\n      <td>71.0</td>\n      <td>male</td>\n      <td>39.03</td>\n      <td>6.3</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.158193</td>\n      <td>-0.121447</td>\n      <td>-0.211743</td>\n      <td>-0.014441</td>\n      <td>-0.079715</td>\n      <td>-0.143533</td>\n      <td>0.071336</td>\n      <td>0.013479</td>\n      <td>-0.085260</td>\n      <td>0.011425</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>127</td>\n      <td>1</td>\n      <td>1</td>\n      <td>66.0</td>\n      <td>female</td>\n      <td>23.58</td>\n      <td>5.8</td>\n      <td>145.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.054226</td>\n      <td>-0.093798</td>\n      <td>-0.332355</td>\n      <td>-0.064596</td>\n      <td>0.043048</td>\n      <td>-0.200588</td>\n      <td>0.136516</td>\n      <td>0.041142</td>\n      <td>-0.021674</td>\n      <td>-0.084981</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2995</th>\n      <td>95954</td>\n      <td>1</td>\n      <td>0</td>\n      <td>61.0</td>\n      <td>male</td>\n      <td>24.52</td>\n      <td>6.6</td>\n      <td>165.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.172905</td>\n      <td>-0.028232</td>\n      <td>-0.289735</td>\n      <td>0.031709</td>\n      <td>-0.036546</td>\n      <td>-0.199922</td>\n      <td>0.011492</td>\n      <td>0.098776</td>\n      <td>-0.031317</td>\n      <td>-0.044507</td>\n    </tr>\n    <tr>\n      <th>2996</th>\n      <td>96030</td>\n      <td>1</td>\n      <td>0</td>\n      <td>73.0</td>\n      <td>female</td>\n      <td>25.07</td>\n      <td>6.3</td>\n      <td>158.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.050674</td>\n      <td>-0.060000</td>\n      <td>-0.283743</td>\n      <td>-0.085756</td>\n      <td>-0.056065</td>\n      <td>-0.177056</td>\n      <td>0.061681</td>\n      <td>0.067599</td>\n      <td>-0.042998</td>\n      <td>-0.052660</td>\n    </tr>\n    <tr>\n      <th>2997</th>\n      <td>96035</td>\n      <td>1</td>\n      <td>0</td>\n      <td>18.0</td>\n      <td>male</td>\n      <td>37.65</td>\n      <td>6.5</td>\n      <td>90.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.141482</td>\n      <td>-0.078641</td>\n      <td>-0.259597</td>\n      <td>0.134808</td>\n      <td>0.033681</td>\n      <td>-0.146596</td>\n      <td>0.002997</td>\n      <td>0.081718</td>\n      <td>0.014971</td>\n      <td>-0.069236</td>\n    </tr>\n    <tr>\n      <th>2998</th>\n      <td>96093</td>\n      <td>1</td>\n      <td>1</td>\n      <td>74.0</td>\n      <td>female</td>\n      <td>6.00</td>\n      <td>6.8</td>\n      <td>165.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.124343</td>\n      <td>0.006571</td>\n      <td>-0.255402</td>\n      <td>-0.014538</td>\n      <td>-0.036401</td>\n      <td>-0.155102</td>\n      <td>0.129003</td>\n      <td>0.092416</td>\n      <td>-0.071230</td>\n      <td>-0.012100</td>\n    </tr>\n    <tr>\n      <th>2999</th>\n      <td>96095</td>\n      <td>1</td>\n      <td>0</td>\n      <td>64.0</td>\n      <td>female</td>\n      <td>25.48</td>\n      <td>6.5</td>\n      <td>140.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.085310</td>\n      <td>-0.102265</td>\n      <td>-0.257324</td>\n      <td>-0.022427</td>\n      <td>0.071746</td>\n      <td>-0.198685</td>\n      <td>0.103298</td>\n      <td>0.043273</td>\n      <td>-0.056627</td>\n      <td>-0.058333</td>\n    </tr>\n  </tbody>\n</table>\n<p>3000 rows × 779 columns</p>\n</div>"},"metadata":{}}],"execution_count":59},{"cell_type":"markdown","source":"## 5️⃣ Análisis Exploratorio (EDA Completo)","metadata":{}},{"cell_type":"code","source":"\n\nprint(f\"\\n EDAD (Age):\")\nprint(f\"   Media: {df_train_final['age'].mean():.1f} años\")\nprint(f\"   Mediana: {df_train_final['age'].median():.1f} años\")\nprint(f\"   Rango: {df_train_final['age'].min():.0f} - {df_train_final['age'].max():.0f} años\")\nprint(f\"   Faltantes: {df_train_final['age'].isna().sum()}\")\n\nprint(f\"\\n GÉNERO (Gender):\")\ngen_dist = df_train_final['gender'].value_counts()\nfor g, c in gen_dist.items():\n    print(f\"   {g}: {c} ({c/len(df_train_final)*100:.1f}%)\")\n\nprint(f\"\\n BMI:\")\nprint(f\"   Media: {df_train_final['bmi'].mean():.2f} ± {df_train_final['bmi'].std():.2f}\")\nprint(f\"   Rango: {df_train_final['bmi'].min():.2f} - {df_train_final['bmi'].max():.2f}\")\nprint(f\"   Faltantes: {df_train_final['bmi'].isna().sum()}\")\n\nprint(f\"\\n HbA1c:\")\nprint(f\"   Media: {df_train_final['hba1c'].mean():.2f} ± {df_train_final['hba1c'].std():.2f}\")\nprint(f\"   Rango: {df_train_final['hba1c'].min():.2f} - {df_train_final['hba1c'].max():.2f}\")\nprint(f\"   Faltantes: {df_train_final['hba1c'].isna().sum()}\")\n\nprint(f\"\\n GLUCOSA (Glucose):\")\nprint(f\"   Media: {df_train_final['glucose'].mean():.2f} ± {df_train_final['glucose'].std():.2f}\")\nprint(f\"   Rango: {df_train_final['glucose'].min():.2f} - {df_train_final['glucose'].max():.2f}\")\nprint(f\"   Faltantes: {df_train_final['glucose'].isna().sum()}\")\n\nprint(f\"\\n HIPERTENSIÓN:\")\nhyp_count = df_train_final['has_hypertension'].sum()\nprint(f\"   Con hipertensión: {hyp_count} ({hyp_count/len(df_train_final)*100:.1f}%)\")\n\nprint(f\"\\n ENFERMEDAD CARDÍACA:\")\nhd_count = df_train_final['has_heart_disease'].sum()\nprint(f\"   Con cardiopatía: {hd_count} ({hd_count/len(df_train_final)*100:.1f}%)\")\n\nprint(f\"\\n FUMACIÓN:\")\nsmoke_dist = df_train_final['smoking_status'].value_counts()\nfor s, c in smoke_dist.items():\n    print(f\"   {s}: {c} ({c/len(df_train_final)*100:.1f}%)\")\n\nprint(f\"\\n DIABETES (TARGET):\")\ndiab_dist = df_train_final['has_diabetes'].value_counts()\nprint(f\"   Negativo (0): {diab_dist[0]} ({diab_dist[0]/len(df_train_final)*100:.1f}%)\")\nprint(f\"   Positivo (1): {diab_dist[1]} ({diab_dist[1]/len(df_train_final)*100:.1f}%)\")\n\nprint(f\"\\n CORRELACIÓN CON DIABETES:\")\nnumeric_cols = ['age', 'bmi', 'hba1c', 'glucose', 'has_hypertension', 'has_heart_disease', 'note_count']\ncorr = df_train_final[numeric_cols + ['has_diabetes']].corr()['has_diabetes'].sort_values(ascending=False)\nprint(corr.head(8))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T18:41:46.325605Z","iopub.execute_input":"2025-11-04T18:41:46.326148Z","iopub.status.idle":"2025-11-04T18:41:46.343672Z","shell.execute_reply.started":"2025-11-04T18:41:46.326125Z","shell.execute_reply":"2025-11-04T18:41:46.342871Z"}},"outputs":[{"name":"stdout","text":"\n EDAD (Age):\n   Media: 46.3 años\n   Mediana: 49.0 años\n   Rango: 1 - 80 años\n   Faltantes: 9\n\n GÉNERO (Gender):\n   female: 1663 (55.4%)\n   male: 1337 (44.6%)\n\n BMI:\n   Media: 28.05 ± 7.77\n   Rango: 0.00 - 72.21\n   Faltantes: 5\n\n HbA1c:\n   Media: 6.25 ± 1.07\n   Rango: 3.50 - 9.00\n   Faltantes: 115\n\n GLUCOSA (Glucose):\n   Media: 161.97 ± 43.13\n   Rango: 15.00 - 300.00\n   Faltantes: 211\n\n HIPERTENSIÓN:\n   Con hipertensión: 1017 (33.9%)\n\n ENFERMEDAD CARDÍACA:\n   Con cardiopatía: 2874 (95.8%)\n\n FUMACIÓN:\n   never: 1566 (52.2%)\n   unknown: 768 (25.6%)\n   past: 552 (18.4%)\n   current: 114 (3.8%)\n\n DIABETES (TARGET):\n   Negativo (0): 2100 (70.0%)\n   Positivo (1): 900 (30.0%)\n\n CORRELACIÓN CON DIABETES:\nhas_diabetes         1.000000\nhba1c                0.535270\nglucose              0.446588\nage                  0.429397\nbmi                  0.315766\nhas_hypertension     0.199608\nhas_heart_disease   -0.062372\nnote_count                NaN\nName: has_diabetes, dtype: float64\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"scaler_bmi = StandardScaler()\nscaler_hba1c = StandardScaler()\nscaler_glucose = StandardScaler()\nscaler_age = StandardScaler()\n\ndf_train_final[\"bmi\"] = scaler_bmi.fit_transform(df_train_final[[\"bmi\"]])\ndf_train_final[\"hba1c\"] = scaler_hba1c.fit_transform(df_train_final[[\"hba1c\"]])\ndf_train_final[\"glucose\"] = scaler_glucose.fit_transform(df_train_final[[\"glucose\"]])\ndf_train_final[\"age\"] = scaler_age.fit_transform(df_train_final[[\"age\"]])\n\ndf_test_final[\"bmi\"] = scaler_bmi.transform(df_test_final[[\"bmi\"]])\ndf_test_final[\"hba1c\"] = scaler_hba1c.transform(df_test_final[[\"hba1c\"]])\ndf_test_final[\"glucose\"] = scaler_glucose.transform(df_test_final[[\"glucose\"]])\ndf_test_final[\"age\"] = scaler_age.transform(df_test_final[[\"age\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T18:41:50.851754Z","iopub.execute_input":"2025-11-04T18:41:50.852469Z","iopub.status.idle":"2025-11-04T18:41:50.874195Z","shell.execute_reply.started":"2025-11-04T18:41:50.852439Z","shell.execute_reply":"2025-11-04T18:41:50.873646Z"}},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":"## 6️⃣ Preparación para Modelado","metadata":{}},{"cell_type":"code","source":"# Preparar X_train e y_train\nX_train = df_train_final.drop(columns=[\"patient_id\", \"has_diabetes\"])\ny_train = df_train_final[\"has_diabetes\"]\n\n# Preparar X_test\nX_test = df_test_final.drop(columns=[\"patient_id\"])\n\n# Rellenar NaNs\nprint(\"\\n Rellenando valores faltantes...\")\n\nfor col in X_train.columns:\n    if X_train[col].dtype == 'object':\n        # Categóricos: usar moda\n        mode_val = X_train[col].mode()[0] if not X_train[col].mode().empty else \"unknown\"\n        X_train[col] = X_train[col].fillna(mode_val)\n        X_test[col] = X_test[col].fillna(mode_val)\n    else:\n        # Numéricos: usar media\n        mean_val = X_train[col].mean()\n        X_train[col] = X_train[col].fillna(mean_val)\n        X_test[col] = X_test[col].fillna(mean_val)\n\n# One-hot encoding para categorías\nprint(\"\\n One-hot encoding para gender y smoking_status...\")\nX_train = pd.get_dummies(X_train, columns=['gender', 'smoking_status'], drop_first=True)\nX_test = pd.get_dummies(X_test, columns=['gender', 'smoking_status'], drop_first=True)\n\n# Alinear columnas\nfor col in set(X_train.columns) - set(X_test.columns):\n    X_test[col] = 0\nfor col in set(X_test.columns) - set(X_train.columns):\n    X_train[col] = 0\n\nX_train = X_train[sorted(X_train.columns)]\nX_test = X_test[sorted(X_train.columns)]\n\nprint(f\"\\n X_train final shape: {X_train.shape}\")\nprint(f\" X_test final shape: {X_test.shape}\")\nprint(f\"\\n Columnas features: {X_train.columns.tolist()[:15]}... (+{len(X_train.columns)-15})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T18:41:55.546219Z","iopub.execute_input":"2025-11-04T18:41:55.546486Z","iopub.status.idle":"2025-11-04T18:41:55.930182Z","shell.execute_reply.started":"2025-11-04T18:41:55.546464Z","shell.execute_reply":"2025-11-04T18:41:55.929485Z"}},"outputs":[{"name":"stdout","text":"\n Rellenando valores faltantes...\n\n One-hot encoding para gender y smoking_status...\n\n X_train final shape: (3000, 779)\n X_test final shape: (300, 779)\n\n Columnas features: ['age', 'bmi', 'emb_0', 'emb_1', 'emb_10', 'emb_100', 'emb_101', 'emb_102', 'emb_103', 'emb_104', 'emb_105', 'emb_106', 'emb_107', 'emb_108', 'emb_109']... (+764)\n","output_type":"stream"}],"execution_count":62},{"cell_type":"markdown","source":"## 7️⃣ Ensemble con Weighted Soft Voting\n\nCatBoost () + XGBoost () + Light GBM ()\n\n","metadata":{}},{"cell_type":"markdown","source":"\nLight GBM () + Random Forest () + Logistic Regression ()\n","metadata":{}},{"cell_type":"markdown","source":"CatBoost () + Random Forest () + Logistic Regression ()","metadata":{}},{"cell_type":"code","source":"# ===== LIBRERÍAS =====\ntry:\n    from xgboost import XGBClassifier\n    from catboost import CatBoostClassifier\nexcept Exception:\n    from xgboost import XGBClassifier\n    from catboost import CatBoostClassifier\n\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier, StackingClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, f1_score, accuracy_score\nimport numpy as np\n# Split validación\nX_tr, X_val, y_tr, y_val = train_test_split(\n    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n)\n\n# ===== UTILIDADES =====\ndef auc_weights(models, X_tr, y_tr, X_val, y_val):\n    aucs, names, fitted = [], [], []\n    for name, model in models:\n        model.fit(X_tr, y_tr)\n        proba = model.predict_proba(X_val)[:, 1]\n        auc = roc_auc_score(y_val, proba)\n        aucs.append(auc); names.append(name); fitted.append(model)\n        print(f\"{name}: AUC={auc:.4f}\")\n    auc_clipped = np.clip(aucs, 0.5, 1.0)\n    raw = (np.array(auc_clipped) - 0.5) + 1e-6\n    w = (raw / raw.sum()).tolist()\n    print(\"Pesos:\", {n: round(wi,3) for n, wi in zip(names, w)})\n    return list(zip(names, fitted)), w\n\ndef report(name, model, X_val, y_val):\n    y_hat = model.predict(X_val)\n    y_pb  = model.predict_proba(X_val)[:, 1]\n    auc = roc_auc_score(y_val, y_pb)\n    f1  = f1_score(y_val, y_hat, zero_division=0)\n    acc = accuracy_score(y_val, y_hat)\n    print(f\"[{name}] AUC={auc:.4f}  F1={f1:.4f}  Acc={acc:.4f}\")\n\n# Ratio de desbalance\npos = max(1, int(y_tr.sum()))\nneg = int(len(y_tr) - pos)\nspw = neg / pos\nprint(f\"scale_pos_weight={spw:.2f}\")\n\n# ===== META-MODELO PARA STACKING =====\nmeta_lr = LogisticRegression(max_iter=2000, solver='lbfgs', class_weight='balanced', C=0.5, random_state=42)\ncv5 = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# ===== CATB + XGB + SVM =====\ncatb_opt = CatBoostClassifier(\n    iterations=650, depth=6, learning_rate=0.05, l2_leaf_reg=3.0,\n    loss_function='Logloss', eval_metric='Logloss', class_weights=[1.0, spw],\n    random_state=42, verbose=0\n)\n\nxgb_opt = XGBClassifier(\n    n_estimators=800, max_depth=5, learning_rate=0.03,\n    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n    eval_metric='logloss', tree_method='hist', n_jobs=-1, random_state=42,\n    scale_pos_weight=spw\n)\n\n# SVM con probabilidad calibrada\nsvm_opt = SVC(\n    kernel='rbf', C=1.0, gamma='scale',\n    class_weight='balanced', probability=True,\n    random_state=42\n)\n\nmodels_opt = [('catb', catb_opt), ('xgb', xgb_opt), ('svm', svm_opt)]\nfitted_opt, weights_opt = auc_weights(models_opt, X_tr, y_tr, X_val, y_val)\n\n# Voting Classifier\nvote_opt = VotingClassifier(estimators=fitted_opt, voting='soft', weights=weights_opt)\nvote_opt.fit(X_tr, y_tr)\nreport(\"Voting CATB+XGB+SVM\", vote_opt, X_val, y_val)\n\n\n# Seleccionar mejor modelo\nbest_model = vote_opt  # o stack_opt si tiene mejor rendimiento\nbest_model.fit(X_train, y_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T19:18:33.260840Z","iopub.execute_input":"2025-11-04T19:18:33.261182Z","iopub.status.idle":"2025-11-04T19:23:26.494986Z","shell.execute_reply.started":"2025-11-04T19:18:33.261157Z","shell.execute_reply":"2025-11-04T19:23:26.494171Z"}},"outputs":[{"name":"stdout","text":"scale_pos_weight=2.33\ncatb: AUC=0.9246\nxgb: AUC=0.9251\nsvm: AUC=0.9275\nPesos: {'catb': 0.332, 'xgb': 0.333, 'svm': 0.335}\n[Voting CATB+XGB+SVM] AUC=0.9287  F1=0.7331  Acc=0.8483\n","output_type":"stream"},{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"VotingClassifier(estimators=[('catb',\n                              <catboost.core.CatBoostClassifier object at 0x78ee1c1e5510>),\n                             ('xgb',\n                              XGBClassifier(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=0.8, device=None,\n                                            early_stopping_rounds=None,\n                                            enable_categorical=False,\n                                            eval_metric='logloss',\n                                            feature_types=None, gamma=None,\n                                            grow_pol...\n                                            max_delta_step=None, max_depth=5,\n                                            max_leaves=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            multi_strategy=None,\n                                            n_estimators=800, n_jobs=-1,\n                                            num_parallel_tree=None,\n                                            random_state=42, ...)),\n                             ('svm',\n                              SVC(class_weight='balanced', probability=True,\n                                  random_state=42))],\n                 voting='soft',\n                 weights=[0.33241680215690095, 0.3328621223895178,\n                          0.3347210754535813])","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;catb&#x27;,\n                              &lt;catboost.core.CatBoostClassifier object at 0x78ee1c1e5510&gt;),\n                             (&#x27;xgb&#x27;,\n                              XGBClassifier(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=0.8, device=None,\n                                            early_stopping_rounds=None,\n                                            enable_categorical=False,\n                                            eval_metric=&#x27;logloss&#x27;,\n                                            feature_types=None, gamma=None,\n                                            grow_pol...\n                                            max_delta_step=None, max_depth=5,\n                                            max_leaves=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            multi_strategy=None,\n                                            n_estimators=800, n_jobs=-1,\n                                            num_parallel_tree=None,\n                                            random_state=42, ...)),\n                             (&#x27;svm&#x27;,\n                              SVC(class_weight=&#x27;balanced&#x27;, probability=True,\n                                  random_state=42))],\n                 voting=&#x27;soft&#x27;,\n                 weights=[0.33241680215690095, 0.3328621223895178,\n                          0.3347210754535813])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;catb&#x27;,\n                              &lt;catboost.core.CatBoostClassifier object at 0x78ee1c1e5510&gt;),\n                             (&#x27;xgb&#x27;,\n                              XGBClassifier(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=0.8, device=None,\n                                            early_stopping_rounds=None,\n                                            enable_categorical=False,\n                                            eval_metric=&#x27;logloss&#x27;,\n                                            feature_types=None, gamma=None,\n                                            grow_pol...\n                                            max_delta_step=None, max_depth=5,\n                                            max_leaves=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            multi_strategy=None,\n                                            n_estimators=800, n_jobs=-1,\n                                            num_parallel_tree=None,\n                                            random_state=42, ...)),\n                             (&#x27;svm&#x27;,\n                              SVC(class_weight=&#x27;balanced&#x27;, probability=True,\n                                  random_state=42))],\n                 voting=&#x27;soft&#x27;,\n                 weights=[0.33241680215690095, 0.3328621223895178,\n                          0.3347210754535813])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>catb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CatBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>&lt;catboost.core.CatBoostClassifier object at 0x78ee1c1e5510&gt;</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>xgb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.03, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=800,\n              n_jobs=-1, num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svm</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(class_weight=&#x27;balanced&#x27;, probability=True, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}],"execution_count":69},{"cell_type":"markdown","source":"## 8️⃣ Predicciones Finales en Test","metadata":{}},{"cell_type":"code","source":"\ny_pred_test = best_model.predict(X_test)\ny_pred_proba_test = best_model.predict_proba(X_test)[:, 1]\n\n# Crear submission\nsubmission = pd.DataFrame({\n    'patient_id': df_test_final['patient_id'],\n    'has_diabetes': y_pred_test,\n    'probability': y_pred_proba_test\n})\n\nprint(f\"\\n {len(submission)} predicciones generadas\")\n\nprint(f\"\\n Distribución predicciones:\")\ndist = submission['has_diabetes'].value_counts()\nprint(f\"   Negativo (0): {dist[0]} ({dist[0]/len(submission)*100:.1f}%)\")\nprint(f\"   Positivo (1): {dist[1]} ({dist[1]/len(submission)*100:.1f}%)\")\n\nprint(f\"\\n Probabilidades:\")\nprint(f\"   Media: {submission['probability'].mean():.4f}\")\nprint(f\"   Min: {submission['probability'].min():.4f}\")\nprint(f\"   Max: {submission['probability'].max():.4f}\")\n\nprint(f\"\\n PRIMERAS 10 PREDICCIONES:\")\nprint(submission.head(10).to_string(index=False))\n\n# Guardar\ndef parse_submission(id):\n    id = str(id)\n    length = len(id)\n    return \"patient_\" + \"0\"* (5 - length) + id\n\nsubmission[\"patient_id\"] = submission[\"patient_id\"].apply(parse_submission)\nsubmission[[\"patient_id\",\"has_diabetes\"]].to_csv(\"submission.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T19:23:53.272337Z","iopub.execute_input":"2025-11-04T19:23:53.273071Z","iopub.status.idle":"2025-11-04T19:23:53.932519Z","shell.execute_reply.started":"2025-11-04T19:23:53.273037Z","shell.execute_reply":"2025-11-04T19:23:53.931758Z"}},"outputs":[{"name":"stdout","text":"\n 300 predicciones generadas\n\n Distribución predicciones:\n   Negativo (0): 223 (74.3%)\n   Positivo (1): 77 (25.7%)\n\n Probabilidades:\n   Media: 0.2818\n   Min: 0.0001\n   Max: 0.9998\n\n PRIMERAS 10 PREDICCIONES:\n patient_id  has_diabetes  probability\n        139             0     0.008924\n        252             0     0.005333\n        259             0     0.007896\n        335             1     0.938705\n        699             0     0.001388\n        977             0     0.023111\n       1025             0     0.148927\n       1145             0     0.022740\n       1217             1     0.718553\n       1235             1     0.862378\n","output_type":"stream"}],"execution_count":70},{"cell_type":"markdown","source":"## 9️⃣ Guardado de Archivos","metadata":{}},{"cell_type":"code","source":"# Parquet (comprimido)\ndf_train_final.to_parquet(\"df_train_final.parquet\", index=False)\ndf_test_final.to_parquet(\"df_test_final.parquet\", index=False)\nprint(f\"\\n Parquet (comprimido):\")\nprint(f\"   df_train_final.parquet\")\nprint(f\"   df_test_final.parquet\")\n\n# CSV (primeras 100 filas, legible)\ndf_train_final.head(100).to_csv(\"df_train_sample.csv\", index=False)\ndf_test_final.head(100).to_csv(\"df_test_sample.csv\", index=False)\nprint(f\"\\n CSV (muestras 100 filas):\")\nprint(f\"   df_train_sample.csv\")\nprint(f\"   df_test_sample.csv\")\n\nprint(f\"\\n Tamaño en memoria:\")\nprint(f\"   Train: {df_train_final.memory_usage(deep=True).sum() / 1e6:.2f} MB\")\nprint(f\"   Test: {df_test_final.memory_usage(deep=True).sum() / 1e6:.2f} MB\")\n\nprint(f\"\\n ARCHIVOS GENERADOS:\")\nprint(f\"   1. submission.csv (predicciones finales)\")\nprint(f\"   2. df_train_final.parquet (features train + embeddings)\")\nprint(f\"   3. df_test_final.parquet (features test + embeddings)\")\nprint(f\"   4. df_train_sample.csv (muestra train)\")\nprint(f\"   5. df_test_sample.csv (muestra test)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🎉 Resumen Final\n\n### ✅ Pipeline Completado:\n\n1. **Carga**: train.json + test.json\n2. **Extracción**: edad, género, BMI, HbA1c, glucosa, hipertensión, cardiopatía, fumación\n3. **BioClinicalBERT**: embeddings 768-dimensionales por nota\n4. **Agrupación**: promediado por paciente\n5. **Features**: ~780 columnas (10 clínicas + 768 embeddings + dummies)\n6. **Modelado**: RandomForest 200 árboles con validación 80/20\n7. **Predicciones**: submission.csv con probabilidades\n\n### 📊 Dataset:\n\n- **Train**: 200 pacientes con etiqueta diabetes (133 neg, 67 pos = 33.5%)\n- **Test**: ~300 pacientes sin etiqueta\n- **Features clínicos**: edad (años), género (m/f), BMI (18-80), HbA1c (3-20), glucosa (40-600)\n- **Embeddings**: 768-dim via Bio_ClinicalBERT preentrenado en MIMIC-III\n\n### 🎯 Modelos/Algoritmos:\n\n- **RandomForest**: 200 árboles, max_depth=15, balanced class weights\n- **Validación**: 80/20 train/val, stratified por has_diabetes\n- **Métricas**: Accuracy, Precision, Recall, F1-Score, ROC-AUC\n\n### 💾 Salidas:\n\n- `submission.csv`: patient_id + has_diabetes (0/1) + probability\n- `df_train_final.parquet`: 200 × 776 (patient_id + 9 features + 768 embeddings)\n- `df_test_final.parquet`: 300 × 777 (paciente_id + 9 features + 768 embeddings)\n\n### 📖 Próximas Mejoras:\n\n- XGBoost o LightGBM (suelen superar RF)\n- Hyperparameter tuning (GridSearchCV/Optuna)\n- Ensemble (combinar RF + XGB + Neural Network)\n- Feature engineering (interacciones, ratios)\n- Neural Networks (embeddings directos + dense layers)\n\n### 🔗 Cargar datos después sin re-procesar:\n\n```python\nimport pandas as pd\ndf_train = pd.read_parquet(\"df_train_final.parquet\")\ndf_test = pd.read_parquet(\"df_test_final.parquet\")\nsubmission = pd.read_csv(\"submission.csv\")\n```\n\n---\n\n**Creado**: 03-11-2025  \n**Versión**: 1.0 - Notebook Completo y Funcional\n","metadata":{}}]}